---
Pr-id: MoneyLab
P-id: INC Reader
A-id: 10
Type: article
Book-type: anthology
Anthology item: article
Item-id: unique no.
Article-title: title of the article
Article-status: accepted
Author: name(s) of author(s)
Author-email:   corresponding address
Author-bio:  about the author
Abstract:   short description of the article (100 words)
Keywords:   50 keywords for search and indexing
Rights: CC BY-NC 4.0
...


> **Access to Data on Disinformation within the** **Code of Practice on
> Disinformation **

*Michalina Kowala*

# From Discretion to Obligation? The Researchers’ Access to Data on Disinformation in the Framework of the Code of Practice on Disinformation

The phenomenon of disinformation is not new. As far back as antiquity,
Julius Caesar used the tools of propaganda to demonstrate his power in
order to convert people to the Roman way of life.^1^ Today,
disinformation, defined as ‘verifiably false or misleading information
that is created, presented and disseminated for economic gain or to
intentionally deceive the public, and may cause public harm’,^2^ is
propagated above all in the digital environment. It contributes to
polarization and spread of extreme ideas,^3^ leading to rise in populism
and major social tensions. It ‘undermines trust in institutions and in
traditional and digital media, and hinders citizens’ ability to make
informed decisions’.^4^ It impacts the policymaking process by giving a
distorted image of the actions of public authorities.

Disinformation has intensified in the run-up to the presidential
elections in the United States in 2016 and the Brexit referendum.
According to 83 per cent of Europeans, this phenomenon constitutes a
threat to democracy.^5^ Spread mainly by online platforms,^6^ it
required an immediate legal response at the European Union (EU) level.
The Code of Practice on Disinformation, established in 2018, has been
the first such framework worldwide, setting out the commitments by
platforms and industry to fight disinformation. One of its objectives
was to empower the research community. However, the assessment of the
implementation of the commitments enshrined therein conducted by the
European Commission, the European Regulators Group for Audiovisual Media
Services (ERGA), and other independent consultancy bodies revealed the
multitude of shortcomings. To address them, the strengthened Code of
Practice on Disinformation was adopted in 2022. The objective of this
chapter is to discuss the changes in data access for researchers
studying disinformation since the adoption of the new code.

First, the framework of the researchers’ access to data provided in the
2018 code will be presented. Second, I will refer to my research
experience in accessing data from Facebook through the mechanisms
implemented by the platform following the adoption of the 2018 code.
Third, I will discuss the commitments included in the 2022 code
regarding researchers’ access to data. I will look at the types of data
that researchers are entitled to access, the process of applying for and
obtaining data, and the adopted enforcement mechanisms. Further, the
focus will be on the tools that Facebook has put in place to implement
the commitments included in the new code. I will once again refer to my
research experience in order to assess whether my goals would be
achievable under the 2022 code. It will involve the comparative analysis
between the code (voluntary initiatives) and binding law (notably the
Digital Services Act \[DSA\], 2022). The objective of the chapter is
also to determine how the relationship between researchers and the
platform as regards the access to data on disinformation has changed and
what characterizes it today. I make use of the autoethnography method of
research. It allows for the assessment of the tools made available by
Facebook as a result of the implementation of the codes and leads to the
determination of the scope of researchers’ access to data on
disinformation. I decided to choose Facebook^7^ to study the mechanisms
for granting researchers’ access to data since it was one of the first
signatories of the 2018 code. Moreover, it is considered as the most
used social network for news^8^ and ‘the worst perpetrator’ as regards
the spread of disinformation.^9^

# The 2018 Code of Practice on Disinformation

The European Commission in 2018 declared that

> there are growing expectations that online platforms should not only
> comply with legal obligations under the EU and national law but also
> act with appropriate responsibility in views of their central role so
> as to ensure a safe online environment, to protect users from
> disinformation, and to offer users exposure to different political
> views.^10^

It called upon platforms to step up their efforts to tackle online
disinformation. The European Commission considered that self-regulation
could contribute to these efforts, provided it is effectively
implemented and monitored. To this end, the commission supported the
development of the Code of Practice on Disinformation,^11^ which was
signed in October 2018. It constitutes a set of standards agreed by
online platforms, tech companies, and representatives of the advertising
industry to address the spread of disinformation.

## The 2018 Code: An Instrument of Soft Law 

The use of the soft law is often explained by the greater ease with
which stakeholders can formulate and reach agreement, as well as by
lower administrative costs. The adoption of such an instrument is seen
in some cases as a temporary alternative to binding legislation, which
may already be in the pipeline.^12^ Furthermore, soft law constitutes a
mean of stimulating progress and is preferred when member states have
considerable interests that they do not wish to jeopardize. It is often
proposed to regulate the online environment, which is characterized by
rapid technological change.^13^ Contrary to hard law, it contains
non-binding measures that cannot be legally enforced and is of voluntary
nature.

## Commitments under the 2018 Code 

The code consists of commitments divided into five groups and related to
following five areas: (*a*) scrutiny of ad placements, (*b*) political
advertising and issue-based advertising, (*c*) integrity of services,
(*d*) empowering consumers, and (*e*) empowering the research community.
It bound only its signatories who were free to select the commitments
they wanted to sign up for. Due to the nature of the code, no mechanism
of enforcement or for action in case of non-compliance had been
provided.^14^

As regards the empowerment of researchers with tools enabling the
scrutiny of the phenomenon of disinformation and measures implemented by
platforms to address it, the signatories of the 2018 code committed to
taking the reasonable measures to enable privacy-compliant access to
data for research activities. They recognized the purpose of scientific
cooperation and committed to provide relevant data on the functioning of
their services, including data for independent investigation by academic
researchers and general information on algorithms.^15^ In order to
fulfil these commitments, signatories declared their support for ‘good
faith independent’ efforts to track disinformation and to understand its
impact. This support included sharing privacy-protected datasets,
undertaking joint research, or partnering with academics and civil
society organizations.^16^ To measure and monitor the code’s
effectiveness, the signatories committed to write an annual account of
their work to counter disinformation. However, no specific reporting
scheme has been established for this purpose, and neither the metrics
that should be reported by the signatories. The latter agreed to
cooperate with the European Commission, including making available
appropriate information upon request or responding to the commission’s
questions and consultations.

**Researchers’ Access to Data under the 2018 **

# Code of Practice on Disinformation 

In 2020, I conducted research on measures to combat disinformation
introduced by Facebook. My objective was to assess whether the
implemented mechanisms corresponded to, and could contribute to, the
achieving of specific goals enshrined in the 2018 code.^17^ I decided to
analyse the code through the lens of my personal experience as a
researcher examining the issues related to disinformation. I examined
the data provided in the reports submitted monthly and yearly by the
platform to the European Commission as well as their assessment
conducted by the latter.^18^ I had limited myself to only this source of
information.

I faced several difficulties in establishing a complete landscape of
implemented measures based on data supplied by the platform. In many
cases, the measures that were presented in the reports did not even
indirectly address the problem of disinformation or were not
specifically tailored to tackle this issue.^19^ The platform reported on
harmful practices and adopted solutions selectively, by referring to the
global level, and sometimes, but rarely, to the European level or to
certain member states without any justification for such a mode of
communication. This made it difficult, if not impossible, to assess the
risks and the adequacy of the measures implemented to address them.
Although the reports followed the structure of the code and referred to
the measures taken in relation to the five pillars included therein,^20^
the lack of an established structure and the specific metrics,
indicators, or elements that should be communicated left platforms and,
in the discussed case, Facebook absolutely free to decide on the
presented data and the way it will be done. Therefore, the researcher
could not follow the development of the implemented measures or assess
its effectiveness. This was due to several reasons, including
inconsistent presentation of data,^21^ the repetition of the same data
over several months, the presentation of data for a different country
each month for a single programme or policy, or the restriction to
general statements and the failure to present any figures or
percentages.

My impressions as regards the access to data on disinformation in the
framework of the 2018 code were reflected in other research projects and
reports. The EU DisinfoLab pointed to the complexity of the reports
published by platforms and the difficulty in making the comparison
between them. According to the organization, it hindered the meaningful
analysis of the measures adopted in the context of the disinformation on
COVID-19.^22^ It was due to the own reporting style of each platform
filling in the metrics according to its preferences, due to the lack of
specific data such as country-specific metrics, especially regarding the
audience of disinformation (clickthrough rate, and so on), or due to the
lack of uniform presentation as regards all the countries where the
measures have been implemented.^23^ The EU Disinfo Lab called for more
detailed guidelines on common metrics and streamlined reporting, which
were needed to allow for meaningful comparison of platforms’ responses
to disinformation.^24^ In the assessment of the implementation of the
2018 code conducted by the European Commission, the need for ‘more
consistent reporting adhering to certain minimum information standards
that could allow for an even better assessment of the effectiveness of
the implementation of the Code’^25^ was expressed. Moreover, it has been
pointed out that ‘the independent auditing of the data delivered by the
platforms in their reports could eliminate the debate on whether this
data is correct and representative’.^26^

Indeed, in the 2018 code, the signatories did not commit to set up an
independent body to ensure the transparency of the presented
information. The European Commission was the only interlocutor of
platforms. Since no enforcement measures have been foreseen, it could
only respond to signatories’ reports by encouraging the provision of
more granular data, by expressing its concerns, by urging to take
further action, or by regretting that the signatories did not supply
sufficient information.^27^ However, the platforms’ response varied
depending on whether the provision of specific data corresponded to its
transparency policies.

Although the essential role of researchers in providing the proper
understanding of the phenomenon of disinformation and in contributing to
the development of risk-mitigation mechanisms has been recognized in the
2018 code, the assessment of the implementation of the commitments
contained therein reveals multiple shortcomings.^28^ The incomplete,
inconsistent, out-of-context, and selective provision of data by
platforms and the lack of an enforcement mechanism made researchers
entirely dependent on platforms. The very act of data provision, and the
way in which it was done, was mostly based on voluntariness and
discretion of platforms. Researchers were unable to access information
that would allow them to reconstruct the context, to establish an
overall picture, to determine the cause–effect sequence, or to assess
the effectiveness of the implemented measures. It also influenced the
quality and the effectiveness of my research. The dynamic of the
relationship between researchers and platforms was marked by
one-sidedness and the powerlessness of the latter.

# The 2022 Code of Practice on Disinformation 

The new code,^29^ signed in June 2022, intends to address these
shortcomings. It constitutes an answer to the calls to reinforce the
2018 code in areas such as larger participation of platforms with more
tailored commitments, demonetization of disinformation, provision of a
comprehensive coverage of forms of manipulative behaviour, empowering
users to flag disinformation, as well as increase in the coverage of
fact-checking and access to data to researchers and robust monitoring
framework.^30^ The purpose of the 2022 code is to become a more
effective tool for countering disinformation. It was issued based on the
guidance provided by European Commission^31^ and took into account the
proposal (at that time) for the DSA in the regulatory framework of which
the code would be implemented. However, as underlined by the commission,
‘The 2022 Code of Practice is the result of the work carried out by the
signatories. It is for the signatories to decide which commitments they
sign up to and it is their responsibility to ensure the effectiveness of
their commitments’ implementation.’^32^

The new code is a part of a broader regulatory framework. It is aligned
with the DSA. The latter, adopted in October 2022, constitutes a common
set of rules on intermediaries’ obligations and accountability across
the single market.^33^ The connection of the code with the DSA entails
specific obligations arising especially for very large online platforms
(VLOPs) for which the code becomes a mitigation measure for systematic
risks, one of which is disinformation.

## Provision of Access to Different Types of Data 

The 2022 code includes commitments to set up a framework for robust
access to platforms’ data by the research community and adequate support
for the researchers’ activities.^34^ To achieve it, different categories
of data were identified and different ways to access them were laid out.
First, platforms committed to provide access to non-personal and
anonymized, aggregated, or manifestly made public, continuous,
real-time, or near-real-time data pertinent to undertake research on
disinformation. Its provision takes place without any application
procedure. Second, they committed to provide data on the signatories’
services such as accounts belonging to public figures (for example,
elected officials), news outlets, and government accounts. In this case,
the provision of data occurs through a specific procedure. The access
should be provided through automated means such as application
programming interface (APIs) or other open and accessible technical
solutions. Third, the 2022 code provides a governance structure for
access to data for research purposes requiring additional scrutiny. It
will be interesting to see whether the researcher will be able to expect
a specific action from platforms in terms of the requested data, such as
collection, comparison, and summary of data or whether the role of
platforms will be limited to providing data in raw form only.

## Involvement of Third-Party Body 

The relevant signatories declared to set up, fund, and cooperate with a
third-party body. Its role will be to vet researchers willing to
scrutinize disinformation.^35^ Once vetted, they should be able to
access the personal data shared by signatories in accordance with
protocols to be defined by the independent third-party body. Signatories
committed to also ‘support good faith research into disinformation’ that
involves their services by maintaining an open dialogue with researchers
to keep track of the types of data that are likely to be in demand and
by ensuring transparency on data types that are currently made available
to researchers across Europe.^36^ Finally, signatories committed to
conduct research based on transparent methodology and ethical standards,
as well as to share datasets, research findings, and methodologies with
relevant audiences.^37^

The establishment of the third-party body to help oversee and even
implement the processes envisioned by the code was already recommended
in relation to the 2018 code.^38^ Its role, according to the 2022 code,
will be to vet researchers and to determine who should have access to
data requiring additional scrutiny. This should eliminate platform
discretion when deciding who should be granted access to data. Although
granting access still ultimately depends on signatories, they have
committed to cooperate with the said body. The question arises as to the
scope of this cooperation. It is not clear whether the role of the
third-party body would consist also of checking whether the data
provided by platforms corresponds with the one requested by the
researcher. It does not stem from the commitments included in the 2022
code as to whether the researcher will have a possibility of appeal when
the data would not correspond to what they have requested and to whom
they could turn in such a case. It is not clear whether the
consideration of appeals would also be the role of the third-party
body.^39^

## Alignment of the Code with the DSA

The 2022 code, like its predecessor, is voluntary. It means that the
adherence to the code does not imply legal consequences in case of the
lack of implementation of the commitments included therein. However, it
is considered as a possible risk-mitigation measure under Article 35 of
the DSA. For VLOPs,^40^ this means that the complete abandonment of the
adoption of voluntary measures is not possible in practice.

The Code of Practice on Disinformation is considered as a code of
conduct under Article 45 of the DSA and disinformation is considered as
a systematic risk.^41^ The latter, according to Recital 79 of the DSA,
can stem from ‘the design, functioning and use of the services of very
large online platforms, as well as from potential misuses by the
recipients of the service’. The EU legislator has distinguished four
categories of systematic risks and classified the dissemination of
disinformation in one of them. Platforms should diligently mitigate the
systemic risks identified in the risk assessments, in observance of
fundamental rights, for example, by initiating and joining the codes of
conduct.

According to Article 45 of the DSA where significant systemic risk
emerges, the European Commission may invite the providers of VLOPs
concerned or the providers of very large online search engines (VLOSEs)
concerned and other actors to participate in the drawing up of codes of
conduct, including setting out of commitments to take specific
risk-mitigation measures, as well as a regular reporting framework on
any measures taken and their outcomes. The commission should also aim to
ensure that participants report on any measures taken and their
outcomes. According to Recital 104 of the DSA, the refusal without
proper explanations by an online platform of the commission’s invitation
to participate in the application of such a code of conduct could be
taken into account, where relevant, when determining whether the online
platform has infringed the obligations laid down by the DSA. It has been
specified in the same recital that the mere fact of participating in and
implementing a given code of conduct should not in itself presume
compliance with the DSA. Therefore, the signatories must be proactive in
addressing the systemic risks, including the circulation of
disinformation.^42^

The 2018 code was criticized for the lack of enforcement measures.^43^
As for the new code, the enforcement mechanisms can be identified as a
result of its alignment with the DSA. According to Article 37 of the
DSA, in order to monitor and assess the compliance of VLOPs with certain
DSA obligations and, where relevant, the commitments undertaken pursuant
to *codes of conduct*, the independent audits should be conducted at
least once a year.^44^

The signatories of the 2018 code could choose the commitments they
wanted to sign up for. As for its updated version, the signatories
agreed to sign up for commitments that are relevant to the products,
activities, and services they offer. In case when they do not sign up to
a commitment because it is not relevant or pertinent to their services,
they will explain the reasons for this. It should be noted that this
mechanism was not foreseen in the previous version of the code. In my
opinion, it obliges the signatories to be transparent when it comes to
subscription to the commitments and their further implementation and
leaves less room for discretion in this respect.

## Reporting Scheme 

To address the problem of fragmentation and lack of uniformity of
reported data, the 2022 code includes an intensified reporting scheme.
Signatories committed to provide the baseline reports to the European
Commission within one month after the end of the implementation
period.^45^ After that, VLOPs committed to provide regular reporting on
service level indicators and qualitative reporting elements every six
months and other signatories yearly, at service and memberstate
level.^46^ This should allow for a thorough assessment of the extent of
the code’s implementation. VLOPs are confronted with more demanding
requirements as to the frequency of reporting since they are considered
as posing particular risk in dissemination of illegal content and
societal harms.^47^

Signatories also commit to participate in a permanent task force^48^
chaired by the European Commission and including representatives from
the European Digital Media Observatory (EDMO), the ERGA, and the
European External Action Service (EEASS). The task force’s role is to
establish the harmonized reporting templates for the code’s
implementation^49^ which the signatories undertake to apply.^50^ In
February 2023,^51^ the signatories published their first baseline
reports on how the 2022 code’s commitments are implemented.^52^

The new reporting mechanism clearly determines the elements that should
be presented and the way in which this should be done. In theory, and
from the perspective of researchers studying the problem of
disinformation, it should enable access to more specific data that can
be analysed and compared. The signatories should also be less likely to
present the results of activities not aimed at combating disinformation
by plugging them in as such or to provide data selectively, for example,
only for a few member states, since they should follow the agreed
reporting scheme and include the said indicators. A specific
infrastructure regarding reporting has been put in place, and the
establishment of the Task Force gives hope that that discretionary
reporting will be reduced or even eliminated.

**Researchers’ Access to Data under the 2022 **

# Code of Practice on Disinformation 

After the brief analysis of the commitments and measures enshrined in
the 2022 code, it is worth discussing the instruments made available for
researchers by Facebook as a result of its implementation. I will refer
also to my original research project aimed at assessing whether
implemented mechanism by the platform contributes to achieving the goals
enshrined in the code. I will analyse whether my research purpose would
be successful under the 2022 code. My objective is to verify what, if
anything, has changed in the 2022 code that would allow achieving the
objectives of the code more effectively.

## Access to Data Not Requiring the Application Process

With regard to the commitment to provide access to non-personal data and
anonymized, aggregated, or manifestly made public data for research
purposes on disinformation, Facebook, in its recently published baseline
report, refers to the data provided in its ‘Community Standards
Enforcement Report’,^53^ ‘Widely Viewed Content Report’,^54^ and
‘Quarterly Adversarial Threat Report’.^55^ It declares to support
independent research that will enhance understanding of the impact that
platforms like Facebook have on society. Moreover, it claims that its
policies are based on years of experience and expertise in trust and
safety, combined with external input from experts around the world.^56^
However, although the access to provided data should serve research on
disinformation, there is little data on this particular problem in the
indicated reports. The platform repeats and rephrases what has already
been said publicly and adds nothing new.

## Access to Data Requiring the Application Process

Regarding access to data that requires an application process,^57^
Facebook refers to a tool called CrowdTangle. Launched in 2019, it is
described as a content discovery and social monitoring platform that
provides access to a small subset of public data on Facebook.
Researchers used CrowdTangle to study a variety of key topics of social
interest, including misinformation, elections, and societal impact of
COVID-19.^58^ To gain access to this data, researchers must complete an
application process.

CrowdTangle has been considered as an unparalleled tool for ‘analysing
trends, tracking article sources and understanding virality on
Facebook’.^59^ In general, it is regarded as a vital instrument for
researching Facebook’s transparency.^60^ Despite some rumours that
Facebook plans to phase it out,^61^ on the CrowdTangle website updated
at the beginning of February 2023, the platform declared continued
support for the research community and its plans to update this page and
expand its support into new areas.^62^ It is also indicated that the
university researchers (faculty students, PhD students, postdoctoral
research fellows) focused on topics such as misinformation, elections,
or COVID-19 are prioritized regarding the provision of access to the
CrowdTangle interface and APIs, as well as to trainings and resources.

I attempted to apply for this access twice, in December 2022 and in
February 2023. In both cases, the result was the same. I was informed
that specific research topics were prioritized and that if my research
falls outside that scope, I may not be onboarded. I was advised to wait
for further information.^63^ Unfortunately, my request has not been
approved despite the fact that I am a PhD student conducting research on
disinformation and presented my research problem in detail as well as
the purpose of having access to CrowdTangle since I received neither an
answer nor access to CrowdTangle. I was not informed about the reasons
Facebook refused my access request. However, it appears that I am not
the only researcher who has been denied access to CrowdTangle’s
resources. Facebook in 2022 stopped accepting any new-user application
due to ‘staffing constraints’,^64^ and it seems that it has not dealt
with the problem yet, since academics report that new applications are
still not accepted.^65^

As to the commitment to provide vetted researchers with access to data
necessary to undertake research on disinformation by developing,
funding, and cooperating with an independent third-party body, Facebook
declared to have actively engaged in the EDMO Working Group on Platform
to Researcher Data Sharing to develop standardized processes for sharing
data with researchers. Further implementation measures are planned.^66^
The baseline report specifies that the technical standards and
safeguards as well as standards for researchers’ eligibility still need
to be established.^67^ It is expected that another code of conduct will
be adopted for both platforms and researchers to balance the need for
more transparency and research with the protection of personal data.

The third-party body has not been set up yet and it is difficult to
predict when it will be done. In theory, it should have important
resources, independence, and a strong mandate. However, practical
questions regarding how the cooperation will evolve and whether the
involvement of an independent body will reduce the arbitrariness of
platforms’ decisions in the context of sharing personal data remain
unanswered. Moreover, in the face of such a threat of disinformation and
given that the code was adopted in June 2022, the waiting time for the
implementation of some of its measures is long. The code, unfortunately,
still remains ineffective as regards some commitments and measures
included therein.

# Achievability of the Research Goals under the 2022 Code of Practice on Disinformation 

To analyse whether my original research purpose would be successful
under the 2022 code, the evolution of two factors, namely *discretion*
of platforms and *lack of enforcement measures*, which contributed to
the non-achievability of my research goal under the 2018 code, should be
examined.

## Discretion in Reporting of Data 

Platforms’ discretion is likely to be limited under the new Code in many
aspects. The signatories provided assurance of its ability to be
transparent as to the subscription to the commitments and their further
implementation, which means that they have to explain and justify why
they decided not to sign up for certain commitments if it was the case.
The regular reporting was foreseen in both versions of the code – with
the difference that in the new code, the frequency of reporting has been
increased in relation to the VLOPs, which allows for a more thorough
scrutiny of the adopted measures. Signatories committed to participate
in a permanent task force whose role is to establish a harmonized
reporting template and to develop the structural indicators to measure
the code’s overall impact on disinformation. This is the most
significant change when it comes to limiting the discretion of platforms
under the 2022 code, since this is the incomplete, inconsistent,
out-of-context, and selective provision of data by platforms which
hampered many researches on disinformation.

The publication of the first baseline reports in 2023 showed that
unfortunately not all signatories complied with the newly agreed
reporting scheme. X (formerly Twitter) has been criticized for providing
little specific information and no targeted data in relation to its
commitments.^68^ As to Facebook, there are legitimate concerns that the
platform will continue to report on the adopted measures in a
discretionary and incomplete manner. To give some examples from the
report published by the platform, Facebook did not provide any
information as regards the methodology of data measurement or data
concerning the number of academic accounts granted with access to the
CrowdTangle as of January 2023.^69^ Yet it should do so within the
framework of the implementation of measure 26.1 of the code.^70^

## Provision of Access to Different Types of Data 

With regard to the commitment to provide access to non-personal data and
anonymized, aggregated, or manifestly made public data for research
purposes on disinformation, which has been already discussed in the
chapter, Facebook referred to its general policies^71^ but provided
little data on how they translate into access to information on
disinformation, on the scope of this access, or on the kind of data
researchers have access to. These two examples (although an analysis of
the entire report would give rise to more) are the source of doubts as
to whether my research, directed on whether the implementation of the
2022 code contributes to the achievement of its objective, would be
successful, even if only because of the gaps in the report identified
here.

It should, however, be pointed out that the baseline report provided by
Facebook is considerably coherent as regards, for example, the provision
of data per countries,^72^ the lack of which constituted a significant
weakness under the 2018 code. Moreover, platforms, in the new code,
committed to cooperate with members of the task force as regards the
consultation with researchers, the development of the third-party body,
or sharing of datasets, research findings, or methodologies. On the one
hand, giving independent actors a role of intermediaries, watchdogs, and
cooperators with platforms in the discussed field may be a guarantee of
greater transparency. On the other hand, the question as to whether each
platform will engage in this cooperation with the same intensity and
what consequences they will face if they do not do so at all should be
asked. This is not the only question that arises as regards the
compliance with the declared commitments. It is relevant to ask what
consequences and which mechanism will be applicable to platforms if they
do not cooperate with researchers, for example, for not providing data
to those who will be vetted by the third-party body or if they do not
properly fulfil their reporting obligations.

## Enforcement Measures 

Much hope lies in the enforcement measures. Nevertheless, since the code
is of voluntary nature, there were none in the first or in the current
version of the code. The status of the 2022 code is, however, different.
As it has been already explained, the code is understood as a part of
co-regulatory framework foreseen in the DSA which defines certain
objectives and criteria that should be respected and applied in
particular to the VLOPs. According to Article 45 of the DSA, in the case
of systematic failure to comply with the codes of conduct, the European
Commission and the European Board for Digital Services may invite the
signatories to take the necessary action. The EU legislator has not
further specified the term ‘necessary action’, and the rule that the
commission *invite* the signatories to take necessary action suggests
that the intervention of soft nature of the European Commission is
foreseen. In case necessary action is not undertaken by the signatory,
it should be presumed that the provisions on the non-compliance, fines,
and penalties from Articles 73–79 of the DSA will apply.

Moreover, according to Article 37(b) of the DSA, the VLOPs should be
subject to independent audits to assess compliance with any commitments
undertaken while complying with the codes of conduct. The EU legislator
specified that the audits should be effective, efficient, and timely.
The term ‘independent’ has been defined to ensure the transparency of
the auditing process and the credibility of its results. In case the
outcomes of the audit are not positive, the VLOPs should take due
account of the operational recommendations with a view to taking the
necessary measures to implement them.^73^ According to Article 37(6) of
the DSA, where the signatories do not implement the operational
recommendations, they shall justify in the audit implementation report
the reasons for not doing so and set out any alternative measures. If
the audit will not be carried out or if recommendations following the
audit or the said alternative measures will not be implemented, it could
be assumed that the provisions on the non-compliance, fines, and
penalties from Articles 73–79 of the DSA will apply. The question arises
as to what would be the consequences of partial or inaccurate
implementation of recommendations in case it would, for example, hamper
the researchers’ access to information. On 5 May 2023, the European
Commission published the draft of the delegated regulation which was
aimed at setting out the necessary rules for the procedures,
methodology, and templates used for the audits.

The alignment of the 2022 code with the DSA provides the mechanism of
the enforcement of the 2022 code. The use of binding measures from the
DSA in combination with the implementation of the commitments from the
code of voluntary nature focused on the specific problem of
disinformation allows me to believe that my original research conducted
today would have been more successful. However, it should be noted that
the procedure of the enforcement of the code is delimited in a general
way. It lacks the researcher-centred mechanism that would allow him, for
example, to effectively contest platforms’ decisions.

# 

# Conclusion 

The 2018 code was the first, albeit unsuccessful, attempt to provide a
legal framework to address the problem of disinformation. The lack of
success of this voluntary tool lies in the very problem of
disinformation and the business models of online platforms such as
Facebook. Disinformation is a phenomenon difficult to grasp due to the
ever-new ways and techniques of its dissemination and the rapid
development of new technologies. Platforms provide the environment for
its diffusion. The circulation of disinformation brings them increased
traffic, which translates into higher profits. Therefore, they are
reluctant to engage actively in countering this phenomenon.

In the face of this emerging complexity, the adoption of legal measures
to combat disinformation is challenging. The self-regulatory solution
chosen in 2018 offered the possibility of adapting the flexible measures
to different operating systems of the platforms and to target the
specific threats posed by the spread of disinformation. Such instrument
allowed a rapid response tailored to evolving new technologies. However,
this chapter revealed quite a long list of its shortcomings, which,
first, did not allow to effectively combat disinformation^74^ and,
second, did not facilitate research on the phenomenon.

The 2022 code addresses the shortcomings and fills in the gaps of the
2018 code. The commitment to cooperate with researchers and with the
third-party body, as well as other organizations, regarding the
provision of data and the transparency of this process, means that the
discretion of platforms in this regard could be reduced. While
researchers remain dependent on platforms to obtain data, as the latter
are the source, the 2022 code balances this dependency with elements
such as the involvement of intermediaries, a defined reporting scheme,
and enforcement measures. The alignment of the code with the DSA makes
the voluntariness of the implementation of the commitments no longer
unlimited, especially for the VLOPs. The binding law allows for
enforceability of agreed measures and provides a defined liability
scheme in case of non-compliance. Therefore, the combination of the
commitments enshrined in the code focused on the specific problem of
disinformation with the binding law, the DSA, is an added value. It
allows for greater flexibility and provides better mechanisms of control
and ensuring transparency. However, the question of whether such a
combination would translate into a better quality of ongoing research on
disinformation and greater research opportunities in this area should be
asked. I identified some gaps as regards the tools entrusted to
researchers to signal the platforms’ inaction or incomplete fulfilment
of commitments or to challenge their decisions on data provision.
Moreover, the analysis of the first baseline report submitted by
Facebook shows some incompleteness and vagueness in the provision of
data. It may be the first sign that the solutions adopted in the 2022
code aligned with the DSA are not sufficient.

It is necessary to wait for the implementation of all mechanisms to
which the signatories have committed in the 2022 code. Under the 2018
code, researchers were not provided with effective instruments to access
data on disinformation circulating on platforms and to analyse the
measures taken by the latter to counter it. The 2022 code aligned with
the DSA is an important, although for the moment still not sufficient,
step towards ensuring greater data access for researchers studying
disinformation.

# Notes

1.  Garth S. Jowett and Victoria O’Donnell, *Propaganda and Persuasion*
    (Sage Publications, 2012).

2.  ‘Communication from the Commission to the European Parliament, the
    Council, the European Economic and Social Committee and the
    Committee of the Regions, Tackling Online Disinformation: A European
    Approach’, COM (2018) 236 final, European Commission,
    https://eur-lex.europa.eu/
    legal-content/EN/TXT/PDF/?uri=CELEX:52018DC0236 (accessed 28 July
    2023).

3.  J. Bayer, Bernd Holznagel, Katarzyna Lubianiec, Adela Pintea,
    Josephine B. Schmitt, Judit Szakács, and Erik Uszkiewicz,
    ‘Disinformation and

> Propaganda: Impact on the Functioning of the Rule of Law and
> Democratic Processes in the EU and Its Member States’, European
> Parliament, 2021,
> https://www.europarl.europa.eu/RegData/etudes/STUD/2021/653633/
> EXPO\_STU(2021)653633\_EN.pdf (accessed 28 July 2023).

1.  ‘Communication from the Commission’.

2.  ‘Fake News and Disinformation Online’, European Commission, March
    2018, https://europa.eu/eurobarometer/surveys/detail/2183 (accessed
    28 July 2023).

3.  ‘Communication from the Commission’.

4.  Despite the name change of the platform in 2021 to Meta, the name
    Facebook is used throughout this chapter.

5.  Nic Newman, Richard Fletcher, Craig T. Robertson, Kirsten Eddy, and
    Rasmus Kleis Nielsen, *Reuters Institute Digital News Report 2022*
    (Reuters Institute, University of Oxford, 2022).

6.  Mark Travers, ‘Facebook Spreads Fake News Faster than Any Other
    Social Website, According to New Research’, *Forbes*, 21 May 2020,
    https://
    www.forbes.com/sites/traversmark/2020/03/21/facebook-spreadsfake-news-faster-than-any-other-social-website-according-to-newresearch/?sh=3deca4b56e1a
    (accessed 24 February 2023).

7.  ‘Communication from the Commission’.

8.  ‘Communication from the Commission’.

9.  Linda Senden, ‘Soft Law, Self-Regulation and Co-Regulation in
    European Law: Where Do They Meet?’ *Electronic Journal of
    Comparative Law* no. 9 (2005): 1–27, 24.

10. Senden, ‘Soft Law, Self-Regulation and Co-Regulation’, 1, 23.

11. Iva Plasilova, Jordan Hill, Malin Carlberg, Marion Goubet, and
    Richard Procee*,* *Study for Assessment of the Implementation of the
    Code of Practice on Disinformation Final Report*, SMART 2019/0041,
    2019, https://
    www.imap-migration.org/sites/default/files/Publications/2020-07/
    Studyfortheassessmentofthecodeofpracticeagainstdisinformation.pdf
    (accessed 8 February 2023).

12. Code of Practice on Disinformation, 2018,
    https://ec.europa.eu/newsroom/ dae/redirection/document/87534
    (accessed 18 December 2022).

13. The partnerships concluded with academics reported by Google, Meta,
    X (now Twitter), Mozilla, and Microsoft consisted of offering the
    training of fact-checkers, making available of datasets, launching
    campaigns on transparency, or building the infrastructures to
    provider researchers with access to non-personally identifiable
    data. See ‘Analysis Code of Practice Annual Report’, 11–12,
    https://ec.europa.eu/newsroom/dae/document. cfm?doc\_id=62698
    (accessed 28 July 2023).

14. The principal purpose of the code was to identify the actions that
    signatories could put in place in order to address the challenges
    related to ‘disinformation’.

15. See ‘Staff Working Document’, SWD (2020)180 final, 2020, European
    Commission, https://ec.europa.eu/newsroom/dae/document.cfm?doc\_
    id=69212 (accessed 16 February 2023).

16. ‘Staff Working Document’. For example, the platform provided data
    linked to the restriction of misleading advertising, unsupported
    commercial claims, or deceptive business practices which were not
    related to the policies against disinformation.

17. The signatories recognized specific goals which were expressed under
    the form of commitments and divided into five groups and related to
    five following areas: (*a*) scrutiny of ad placements, (*b*)
    political advertising and issue-based advertising, (*c*) integrity
    of services, (*d*) empowering consumers, and (*e*) empowering the
    research community.

18. In the report on September and October 2021 actions, ‘Fighting
    COVID-19 Disinformation, as Regards Supporting Media Literacy in
    Europe’, Facebook reported the results of the ‘Together Against
    Covid-19 Misinformation’ campaign. See ‘Reports on September and
    October Actions: Fighting COVID-19 Disinformation Monitoring
    Programme’, European Commission, 2 December 2021,
    https://digital-strategy.
    ec.europa.eu/en/library/reports-september-and-october-actions-fightingcovid-19-disinformation-monitoring-programme
    (accessed 20 February 2023). In the report on November and December
    2021 actions, ‘Fighting COVID-19 Disinformation’, neither this
    action nor its results are mentioned so the assessment of the
    effectiveness of this action in the long term is not possible. See
    ‘Fighting COVID-19 Disinformation: Reports on November and December
    Actions’, European Commission, 27 January 2022, https://
    digital-strategy.ec.europa.eu/en/library/fighting-covid-19-disinformationreports-november-and-december-actions
    (accessed 20 February 2023).

19. Trisha Meyer, Alexandre Alaphilippe, and Claire Pershan, ‘The Good,
    the Bad and the Ugly: How Platforms are Prioritising Some EU Member
    States in Their COVID-19 Disinformation Responses’, EU DisinfoLab,
    28 April 2021,
    https://www.disinfo.eu/publications/the-good-the-bad-and-the-uglyhow-platforms-are-prioritising-some-eu-member-states-in-their-covid-19disinformation-responses
    (accessed 15 May 2023).

20. Meyer, Alaphilippe, and Pershan, ‘The Good, the Bad and the Ugly’.

21. Meyer, Alaphilippe, and Pershan, ‘The Good, the Bad and the Ugly’.

22. Plasilova, Hill, Carlberg, Goubet, and Procee, *Study for
    Assessment*.

23. Plasilova, Hill, Carlberg, Goubet, and Procee, *Study for
    Assessment*.

24. See, for example, ‘Code of Practice against Disinformation:
    Commission Takes Note of the Progress Made by Online Platforms and
    Urges Them to Step Up Their Efforts’, European Commission, 20 March
    2019, https://
    ec.europa.eu/commission/presscorner/detail/en/STATEMENT\_19\_1757
    (accessed 17 February 2023).

25. ‘Staff Working Document’.

26. The new code relates to the following areas: (*a*) scrutiny of ad
    placements, (*b*) political advertising, (*c*) integrity of
    services, (*d*) empowering users, (*e*) empowering the research
    community, (*f*) empowering the fact-checking community, (*g*)
    transparency centre, (*h*) permanent task force, and (*i*)
    monitoring of the code. See *The Strengthened Code of Practice on
    Disinformation 2022*,
    https://ec.europa.u/newsroom/dae/redirection/document/87585
    (accessed 14 February 2023).

27. ‘Commission Presents Guidance to Strengthen the Code of Practice on
    Disinformation’, 26 May 2021, European Commission,
    https://ec.europa. eu/commission/presscorner/detail/en/ip\_21\_2585
    (accessed 15 December 2024).

28. ‘Guidance on Strengthening the Code of Practice on Disinformation’,
    European Commission, 26 May 2021,
    https://digital-strategy.ec.europa.eu/
    en/library/guidance-strengthening-code-practice-disinformation
    (accessed 28 July 2023).

29. ‘Signatories of the 2022 Strengthened Code of Practice on
    Disinformation’, European Commission, 16 June 2022,
    https://digital-strategy.ec.europa.
    eu/en/library/signatories-2022-strengthened-code-practice-disinformation
    (accessed 28 July 2023).

30. ‘The Digital Services Act: Ensuring a Safe and Accountable Online
    Environment’, European Commission, https://commission.europa.eu/
    strategy-and-policy/priorities-2019-2024/europe-fit-digital-age/digitalservices-act-ensuring-safe-and-accountable-online-environment\_en
    (accessed 28 July 2023).

31. ‘VI. Empowering the Research Community’, in *The Strengthened Code
    of Practice on Disinformation 2022*, 26–30,
    https://ec.europa.eu/newsroom/ dae/redirection/document/87585
    (accessed 14 February 2023).

32. The independent procedure of vetting researchers by a Digital
    Services Coordinator to allow them access to data by VLOPs is
    provided in Article 40 of the DSA. The access to data awarded within
    the framework of Article 40 of the DSA is of general nature and not
    limited to data on disinformation.

33. ‘Commitment 28’, in *The Strengthened Code of Practice on
    Disinformation 2022*,
    https://ec.europa.eu/newsroom/dae/redirection/document/87585
    (accessed 14 February 2023).

34. ‘Commitment 29’, in *The Strengthened Code of Practice on
    Disinformation 2022*,
    https://ec.europa.eu/newsroom/dae/redirection/document/87585
    (accessed 14 February 2023).

35. ‘Report of the European Digital Media Observatory’s Working Group on
    Platform-to-Researcher Data Access’, European Digital Media
    Observatory, 31

> May 2022, https://edmoprod.wpengine.com/wp-content/uploads/2022/02/
> Report-of-the-European-Digital-Media-Observatorys-Working-Group-onPlatform-to-Researcher-Data-Access-2022.pdf
> (accessed 13 February 2023).

1.  To compare, an EU legislator in Article 40 of the DSA foresaw the
    establishment of the Digital Services Coordinator to assess
    compliance of VLOPs with the DSA, to vet researchers, and to
    intermediate as regards the provision of data by VLOPs to
    researchers. Many similarities can be noticed between its role and
    the role of third-party body within the 2022 Code of Practice on
    Disinformation. However, the scope of the third-party body is shaped
    narrowly and relates only to the disinformation’s issues.

2.  Online platforms and online search engines whose number of average
    monthly active recipients of the service in the EU is equal to or
    higher than 45 million. See Article 33 of DSA. In this chapter, I
    refer only to VLOPs and omit the very large search engines (VLSEs).
    See ‘\#FindYourVLOP’, https://
    docs.google.com/spreadsheets/d/1H89uABJZCg0BQlUdpDPE0XBpdtX
    WPGQbwLW4Ug\_hmNo/edit\#gid=1177757099 (accessed 25 February

> 2023).

1.  Point ‘h’ of the preamble to *The Strengthened Code of Practice on
    Disinformation 2022*,
    https://ec.europa.eu/newsroom/dae/redirection/document/87585
    (accessed 14 February 2023).

2.  Point ‘h’ of the preamble to *The Strengthened Code of Practice on
    Disinformation 2022*.

3.  See, for example, ‘ERGA Report on Disinformation: Assessment of the
    Implementation of the Code of Practice’, European Regulators Group
    for Audiovisual Media Services, 2020,
    https://erga-online.eu/wp-content/
    uploads/2020/05/ERGA-2019-report-published-2020-LQ.pdf (accessed 13
    February 2023).

4.  Article 37(1) of the DSA.

5.  Six months after the code’s signature which took place on the 16
    June 2022.

6.  ‘Commitment 40’, in *The Strengthened Code of Practice on
    Disinformation 2022*,
    https://ec.europa.eu/newsroom/dae/redirection/document/87585
    (accessed 14 February 2023).

7.  See ‘The Digital Services Act’.

8.  ‘Commitment 41’, in *The Strengthened Code of Practice on
    Disinformation 2022*,
    https://ec.europa.eu/newsroom/dae/redirection/document/87585
    (accessed 14 February 2023).

9.  ‘Measures 37.2’, in *The Strengthened Code of Practice on
    Disinformation 2022*,
    https://ec.europa.eu/newsroom/dae/redirection/document/87585
    (accessed 14 February 2023).

10. ‘Commitment 43’, in *The Strengthened Code of Practice on
    Disinformation 2022*,
    https://ec.europa.eu/newsroom/dae/redirection/document/87585
    (accessed 14 February 2023).

11. Transparency Centre, https://disinfocode.eu (accessed 14 February
    2023).

12. ‘Signatories of the Code of Practice on Disinformation Deliver Their
    First Baseline Reports in the Transparency Centre’, European
    Commission, 9 February 2023,
    https://digital-strategy.ec.europa.eu/en/news/signatoriescode-practice-disinformation-deliver-their-first-baseline-reportstransparency-centre
    (accessed 14 February 2023).

13. ‘Community Standards Enforcement Report’, Meta,
    https://transparency. fb.com/data/community-standards-enforcement
    (accessed 16 February 2023).

14. ‘Widely Viewed Content Report’, Meta,
    https://transparency.fb.com/pl-pl/ data/widely-viewed-content-report
    (accessed 16 February 2023).

15. ‘Meta’s Adversarial Threat Report’, Meta, November 2022,
    https://about.

> fb.com/news/2022/11/metas-adversarial-threat-report-q3-2022 (accessed
> 16 February 2023).

1.  ‘Code of Practice on Disinformation: Meta Baseline Report’, Meta,
    January 2023, https://disinfocode.eu/reports-archive/?years=2023
    (accessed 16 February 2023).

2.  ‘Measure 26.2’, in *The Strengthened Code of Practice on
    Disinformation 2022*,
    https://ec.europa.eu/newsroom/dae/redirection/document/87585
    (accessed 14 February 2023).

3.  ‘Code of Practice on Disinformation: Meta Baseline Report’.

4.  Seth Smaley, ‘Meta Won’t Comment on Its Plans to Abandon
    CrowdTangle’, Poynter, 18 August 2022,
    https://www.poynter.org/reporting-editing/2022/
    meta-wont-comment-on-its-plans-to-abandon-crowdtangle (accessed 14
    February 2023); Maxime Mohr, ‘Meta va éteindre CrowdTangle, son
    outil de mesure des interactions, SiecleDigital’, SiecleDigital, 29
    June 2022, https://
    siecledigital.fr/2022/06/29/meta-va-eteindre-crowdtangle-son-outil-demesure-des-interactions
    (accessed 14 February 2023).

5.  Gemma B. Mendoza, ‘Why Possible Loss of CrowdTangle Worries
    FactCheckers and Disinformation Researchers’, Rappler, 11 July 2020,
    https://
    www.rappler.com/technology/social-media/disinformation-crowdtangledata-access
    (accessed 16 February 2023).

6.  John Albert, ‘Facebook’s Gutting of CrowdTangle: A Step Backward for
    Platform Transparency’, AlgorithmWatch, 3 August 2022, https://
    algorithmwatch.org/en/crowdtangle-platform-transparency (accessed 15
    February 2023).

7.  Christina Fan, ‘CrowdTangle for Academics and Researchers’,
    CrowdTangle, 2023,
    https://help.crowdtangle.com/en/articles/4302208-crowdtangle-foracademics-and-researchers
    (accessed 15 February 2023).

8.  The exact phrasing of Facebook’s response was as follows: ‘Thank you
    for submitting an application for CrowdTangle access. Please note
    that we are prioritizing specific research topics as noted in the
    application form, and may not be able to onboard you if your
    research falls outside that scope. If we are able to onboard you, we
    will be in touch soon. Thanks, the CrowdTangleTeam.’

9.  ‘Meta Pauses New Users from Joining Analytics Tool CrowdTangle’,
    Reuters, 29 January 2022,
    https://www.reuters.com/technology/meta-pauses-newusers-joining-analytics-tool-crowdtangle-2022-01-29
    (accessed 17 May 2023).

10. John Albert, ‘Platforms’ Promises to Researchers: First Reports
    Missing the Baseline’, AlgorithmWatch, 16 February 2023,
    https://algorithmwatch.org/ en/platforms-promises-to-researchers
    (accessed 27 February 2023).

11. ‘Code of Practice on Disinformation: Meta Baseline Report’.

12. ‘Code of Practice on Disinformation: Meta Baseline Report’.

13. ‘Code of Practice on Disinformation: New Transparency Centre
    Provides Insights and Data on Online Disinformation for the First
    Time’, European Commission, 9 February 2023,
    https://digital-strategy.ec.europa.eu/en/
    news/code-practice-disinformation-new-transparency-centre-providesinsights-and-data-online
    (accessed 14 February 2023).

14. Facebook did not provide data on the number of monthly users, number
    of received applications, number of applications rejected, and so
    on. ‘Code of Practice on Disinformation: Meta Baseline Report’.

15. ‘Measure 26.1’, in *The Strengthened Code of Practice on
    Disinformation 2022*,
    https://ec.europa.eu/newsroom/dae/redirection/document/87585
    (accessed 14 February 2023).

16. ‘Code of Practice on Disinformation: Meta Baseline Report’.

17. As regards measure 18.2 of the 2022 code, which concerns the
    development and enforcement of publicly documented, proportionate
    policies to limit the spread of harmful, false, or misleading
    information. Facebook provided information on contents removed for
    violating the ‘harmful health misinformation’ or voter or census
    interference policies as per each member state and then as per EU as
    a whole. ‘Code of Practice on Disinformation: Meta Baseline Report’.

18. Article 37(6) of the DSA.

19. ‘Left Behind: How Facebook is Neglecting Europe’s Infodemic’, Avaaz,
    20 April 2021,
    https://secure.avaaz.org/campaign/en/facebook\_neglect\_
    europe\_infodemic (accessed 19 May 2023).
