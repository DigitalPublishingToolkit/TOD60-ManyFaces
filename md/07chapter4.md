---
Pr-id: MoneyLab
P-id: INC Reader
A-id: 10
Type: article
Book-type: anthology
Anthology item: article
Item-id: unique no.
Article-title: title of the article
Article-status: accepted
Author: name(s) of author(s)
Author-email:   corresponding address
Author-bio:  about the author
Abstract:   short description of the article (100 words)
Keywords:   50 keywords for search and indexing
Rights: CC BY-NC 4.0
...


**From Rights to Skills**

Data Access for Teaching Data Literacy

*Midas Nouwens *

Researchers and policymakers have converged on the idea that operational
transparency and data access are necessary to create meaningful change
in how digital technologies operate in European societies. The General
Data Protection Regulation (GDPR), 2016, set out to ‘increase
transparency for data subjects’ and ‘enhance control over one’s own
data’ as a way to empower individuals vis-àvis technology companies.^1^
The Digital Services Act (DSA), 2022, attempts to set up a distributed
‘data generation machine’^2^ that will continuously produce highquality
information about the operation and impact of online services in order
to curb systemic risks. The Digital Markets Act (DMA), 2022, and the
Data Act, 2023, aim to increase consumer power and redistribute
data-based value ‘in the hands of relatively few large companies’^3^ by
giving people the right to ‘continuous and real-time access’ to any data
generated by their use of a product or service.^4^

This battery of regulation the European Union (EU) is currently
introducing is an attempt to claim power over the way digital
technologies affect member states’ societies, and the EU’s normative
commitment to human rights means that those efforts have also translated
into the strengthening of existing data rights and the creation of new
ones, albeit without a very clear theory of how those rights translate
into concrete empowerment and change. How does access to data lead to
meaningful transparency, and how does this transparency lead to citizen
empowerment and structural change in the power asymmetries of digital
societies?

The theory of change underlying access rights differs depending on the
context in which it is used. Researchers and activists, for example,
have used it to investigate how technology companies operate, reveal the
(negative) impact that they have, and then disseminate this knowledge
through publications.^5^ The implied theory of change of this
*access–publish–change* model is that evidence of non-compliance or
compliant but harmful behaviour will create enough public pressure for
technology companies to alter their practices. Similarly, lawyers have
used access rights to force technology companies to be transparent about
their data processing practices – mostly in the context of platform
labour – as a way to gather evidence for court cases, using an
*access–litigate–change* approach to force a change in the way they
operate.^6^

One context in which the value of access rights has not been discussed
extensively is education: there is no systematic work on this topic and
only few accounts of people’s pedagogical practice,^7^ let alone a
theory of how access rights in education contribute to structural change
in digital societies (that is, *access– educate–change*). For educators,
social change is often one of the explicit goals of their institutions.
Formal education has a long (if not uncomplicated)^8^ liberal tradition
of enfranchising citizens and as a catalyst for structural evolution,^9^
and more critical views argue that universities have a responsibility to
liberate the oppressed^10^ and create a social class fighting for
equality and justice.^11^ In the context of digital technology, higher
education has tried to affect what our digital societies look like by,
for example, introducing ethics courses for computer scientists^12^ or
teaching using exclusively open source software.^13^ At a higher level
of abstraction, educators across disciplines and education levels have
started to argue that the structural datafication of societies requires
its residents to develop a new kind of literacy: *data literacy*. The
pedagogy of data literacy is still unsettled, and it is in the context
of this broader effort that we imagine access rights might find a place
in education.

This chapter discusses how access rights can be used in higher education
as a pedagogical tool to help students develop *data literacy*, a
quality of mind and set of competences that can be employed when
analysing and reacting to technological phenomena in society, based on
our experiences teaching at a (Danish) university. The goal of this
chapter is to (*a*) showcase another context in which access rights can
be used in a way that might help reconfigure existing power balances
under informational capitalism and (*b*) provide educators with a
conceptual introduction and hands-on guide that they can use to employ
these rights in their own teaching.

First, we will briefly introduce the concept of data literacy and the
scholarship around it. Second, we will review existing data access
rights in the EU. Third, we will present concrete exercises that use
data access and the learning outcomes we believe they resulted in, based
on our own experiences over the past three years. Lastly, we will
discuss the limitations of using access rights in education based on
their legal design and the interpretation of organisations.

# Data Literacy 

Academics critical of the restructuring of societies as a result of
digitalization and datafication have started to advocate that the future
citizen needs a certain kind of literacy to co-shape healthy digital
societies. Such ‘reflexive, active and knowing publics’^14^ would have
the ability to, for example, understand and critically reflect on data
collection,^15^ distrust claims of the objective nature of data,^16^ or
be empowered against harmful algorithmic processing. This
interdisciplinary scholarship is loosely organized around the concept of
data literacy, a term which has also migrated into policy
discussions^17^ and (inter)national educational initiatives.^18^ Various
literature reviews have tried to consolidate the research around data
literacy by evaluating definitions and finding common ground, but the
diversity of work means that even these are necessarily scoped more
narrowly around audiences (for example, educators,^19^ researchers and
librarians,^20^ and citizens^21^).

Variations of the term that have been proposed include ‘big data
literacy’,^22^ ‘algorithmic literacy’,^23^ ‘data infrastructure
literacy’,^24^ ‘data mindset’,^25^ and so on (see Table 4.2 for a
non-exhaustive list of definitions). Inevitably, the definitions and
goals of different authors diverge, so these literacies refer to a
variety of practices situated across a spectrum of epistemologies and
philosophies (from objectivist post-positivism to subjectivist
post-modernism). The various approaches can broadly be separated into
two camps: instrumental definitions and critical definitions (while
keeping the inevitable caveat in mind that there is further diversity
within each of those camps and also works that span across this divide).

Instrumental definitions frame literacy as the ability to create or
process data, often with the implied goal to upskill the labour force in
a datafied society. These prompted more critical and non-technical
perspectives, which see literacy as the ability to question the data’s
neutrality and authority, identify and mitigate various forms of
risk,^26^ and have the vocabulary to participate in debates about
technology design.^27^ Infrastructural definitions expand on the
critical approaches to include the social relations as well as the
technical infrastructures that mediate data creation, extraction, and
processing (for example, algorithms, platforms, business models,
standards, power relations, and governing bodies).^28^ A rough
overarching definition of data literacy could be formulated as
*competences that allow a person to work with and critically reflect on
data and the socio-technical infrastructures surrounding it, with the
goal to immunize the individual against informational harm and empower
them to create alternative data worlds*.

**\
**

**Table 4.1** Terms and definitions related to data literacy

  ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
  > **Source**                               **Term**                       **Definition**
  ------------------------------------------ ------------------------------ -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
  > D’Ignazio and                            Big data literacy              ‘the ability to read, work with, analyze and argue with data’, as well as identifying data collection, understanding algorithmic processing of data, and weighing the impacts of data-driven decisions
  >                                                                         
  > Bhargava (2015)^29^                                                     

  > Bucher (2016)^30^                        Algorithmic imaginary          ‘ways of thinking about what algorithms are, what they should be and how they function’

  > Crusoe (2016)^31^                        Data literacy                  ‘the knowledge of what data are, how they are collected, analyzed, visualized and shared, and … the understanding of how data are applied for benefit or detriment, within the cultural context of security and privacy’

  > Philip, OlivaresPasillas, and Rocha      Racial data literacy           ‘the set of practices that are necessary for an individual to be racially literate about data and data-literate about race’ – for example, ‘examining how societal meanings about race are produced, in part, by the possibilities and constraints in the collection, storage, conversion, manipulation, and representation of data sets’
  >                                                                         
  > (2016)^32^                                                              

  > Gray, Gerlitz, and Bounegru (2018)^33^   Data infrastructure literacy   ‘critical inquiry into datafication, into how datasets are created with certain purposes in mind as well as opening up “infrastructural imagination” … about how they might be created, used and organised differently (or not at all)’

  > D’Ignazio and                            Data mindset                   ‘the ability to think both creatively and critically about what insights and stories might be possible to glean from data’
  >                                                                         
  > Bhargava (2018)^34^                                                     

  > Pangrazi and Selwyn (2019)^35^           Personal data literacy         ‘critical understandings of the reconstitutions and recirculation of data’, being able to identify what personal data is, understand how it is processed, reflect on its implications, use data oneself, and tactically resist, obfuscate, and repurpose data
  ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

(*Contd* )

**\
**

**Table 4.1** (*Contd* )

  ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
  > **Source**                       **Term**                          **Definition**
  ---------------------------------- --------------------------------- ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
  > van Es, Coombs, and Boeschoten   Reflexive digital data analysis   digital data analysis (acquiring, cleaning, and analysing) in which ‘researchers consider their own role in the construction of the data’ and ‘take responsibility to discern how \[the tools and platforms they use\] shape the data’
  >                                                                    
  > (2017)^36^                                                         

  > Sander (2020)^37^                Critical big data literacy        ‘awareness, understanding and ability to critically reflect upon big data collection practices, data uses and the possible risks and implications that come with these practices, as well as the ability to implement this knowledge for a more empowered internet usage’
  ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

*Source*: Collated by the authors from the sources listed in the
‘Sources’ column.

The pedagogy of data literacy is still unsettled, in terms of both
teaching methods and how to evaluate their effectiveness. Scholars who
see data literacy as technical competencies are often more closely
connected to educational disciplines and have done more work to
operationalize data literacy and study the effects of different teaching
approaches.^38^ Systematic reviews show that data literacy is being
taught at educational institutions across all levels.^39^ Courses that
integrate data literacy often use practice-based approaches with
real-world data. Sometimes data literacy is taught as multiple,
successive modules in a stand-alone course, while other times it is
integrated in other courses as part of projects.^40^ Assessments about
the learning outcomes of different pedagogical strategies use either
self-report methods (such as surveys and interviews) where students
reflect and describe their competences or direct measures (such as
competency tests and participant observation).^41^ However, these
assessment tools are rarely validated and scholars are calling for
higher-quality methods.^42^

Critical and infrastructural approaches to teaching data literacy have
fewer explicit studies on methods and learning outcomes, and instead
mostly include high-level suggestions or autoethnographic descriptions
of how data literacy could be taught. The focus is less on programming
and data processing, which means teaching methods also include
custom-built tools and devices (for example, DataBasic^43^ and
Stingray^44^), interactive media (for example, Do Not Track^45^),
embodied representations (for example, acting out datasets^46^ and
algorithms^47^), partnerships with more technically capable peers,^48^
reflective discussions,^49^ and so on. There are few post-hoc
evaluations of the learning outcomes of these methods. Rather,
suggestions of approaches are often based on the reflection of the
teacher(s) after multiple experiences running a workshop or course.

In all the works discussing data literacy pedagogy, there has not been
any systematic reflection on the role that access rights can have,
either as part of exercise design or as a subject of study in its own
right.

# Access Rights in EU Tech Law

Access rights over personal data have existed in Europe in some form
since the 1970s.^50^ They were harmonized for the first time across the
EU in 1995 through the Data Protection Directive, 1995,^51^ and were
subsequently fortified through the GDPR in 2016.^52^ The EU’s recent
DSA, DMA, and Data Act are poised to expand on these rights, although
with different scopes and purposes. We briefly summarize the rights that
could be used by students and teachers here, which is not intended as a
comprehensive legal discussion but instead a gentle and pragmatic
introduction for educators who might be unfamiliar with these rights
(see Table 4.2 for an overview).

Readers should keep in mind that only the access right in the GDPR has
been in effect long enough for us to know something about how it works
in practice, whereas the other regulations are still new and untested.
Many European countries and the EU have also conferred access rights to
citizens over public data through freedom of information legislation,
but since their focus is not primarily about increasing transparency or
control over digital technologies, these are not included.

## Article 15 of the General Data Protection Regulation

The GDPR^53^ is a regulation that governs the processing of personal
data about people in the EU, and one of its missions is to address the
difficulties that people experience to stay in control of their personal
data.^54^ The right of access in Article 15 of the GDPR is one way it
tries to improve this, although it already existed in some form in
previous national^55^ and EU legislation,^56^ making it one of the
oldest rights that people have over their personal data in the EU. The
right should give so-called data subjects access to three things: (*a*)
a confirmation as to whether a data controller has data about them;
(*b*) access to a copy of that data; and (*c*) additional
meta-information, such as for what purpose the data is processed, what
categories the personal data falls into, who else might have access to
this data, how long the data will be stored, where the data came from if
it was not provided by the data subject themselves, and if and how it is
being used for automated decision-making.

**Table 4.2** A simplified overview of data access rights in the EU

  -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
  > **Law**                  **Type of data**                                                            **Right holder**           **Applicable to**                                **Access modality**                                                                                   **Response time**
  -------------------------- --------------------------------------------------------------------------- -------------------------- ------------------------------------------------ ----------------------------------------------------------------------------------------------------- --------------------------------------------
  > GDPR,                    Personal data                                                               Any individual             Data controllers                                 Commonly used electronic form                                                                         30 days
  >                                                                                                                                                                                                                                                                                        
  > Article15                                                                                                                                                                                                                                                                              

  > GDPR,                    Personal data                                                               Any individual             Data controllers                                 Structured, commonly used, and machinereadable format                                                 30 days
  >                                                                                                                                                                                                                                                                                        
  > Article 20                                                                                                                                                                                                                                                                             

  > DSA,                     Data necessary to study systemic risks and mitigation strategies            (Vetted) researchers       Very large online platforms and search engines   Appropriate interfaces specified by researcher, platform, or search engine; real-time (if possible)   15 days
  >                                                                                                                                                                                                                                                                                        
  > Article 40                                                                                                                                                                                                                                                                             

  > DMA, Article 6(9)        Data provided and generated by an end user                                  Users/                     Gatekeepers                                      Effective, continuous, and real-time                                                                  Immediate
                                                                                                                                                                                                                                                                                           
                                                                                                         Authorised third parties                                                                                                                                                          

  > Data Act, Articles 3–4   Product and service data, including metadata necessary for interpretation   Users/                     Data holders                                     Comprehensive,                                                                                        Directly accessible or without undue delay
                                                                                                                                                                                                                                                                                           
                                                                                                         Authorised third parties                                                    structured, commonly used, machinereadable, continuous, and                                           
                                                                                                                                                                                                                                                                                           
                                                                                                                                                                                     real-time                                                                                             
  -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

*Source*: Collated by the authors.

The European Data Protection Board (EDPB), the institution responsible
for making sure the GDPR is consistently applied across the EU, has
provided a practical interpretation of this right that describes how
organizations should respond when people exercise it.^57^ For example,
many data controllers will ask data subjects to use a particular form or
email address when making their request, but the EDPB explains any
request received through a reasonable channel is a valid request, so
people cannot be made to use particular communication channels or
templates. A data controller should also always interpret an access
request as broadly as possible and be as comprehensive as possible with
their response. That response should be sent within a month after the
request was made, unless it is too complicated to do so in that time. If
the identity of the data subject is uncertain (for example, if an access
request is made from a different email address than the one the
controller has on file), the data controller can ask for additional
information, but this information should follow the principle of data
minimization and never be more extensive than is strictly necessary to
confirm the identity of the data subject; asking for copies of passports
or biometric information, which some data controllers have started to
do, is in most cases not necessary or allowed. In terms of the
formatting of the data, there are no strict requirements, but it should
be assumed that if the request is made by electronic means, then the
response should use the same modality in a commonly used electronic
format (which in practice often means CSV or JSON files). These
instructions from the EDPB are intended to protect the right from
constraints that might emerge in practice and to make it as easy as
possible for people to exercise.

## Article 20 of the General Data Protection Regulation 

The right to data portability gives people the right to receive a copy
of their personal data that is processed by automated means (that is,
not paper files) in a structured, commonly used, and machine-readable
format, or have that data sent directly to another data controller. The
right was introduced in the GDPR as a response to the issues individuals
previously had when exercising their access right under the GDPR’s
precursor, the Data Protection Directive:^58^ people would receive their
personal data in whatever format the data controller decided, limiting
how they could manage and reuse the data.

Not all personal data can be requested under this right, but only
personal data that the controller has based on a person’s consent or the
contract they have with them. Personal data processed for, for example,
complying with legal obligations that the controller might have (such as
fraud detection) are not covered. Another limitation is that it only
includes data ‘provided by’ the data subject, although this is
interpreted broadly: it includes not only obvious things directly
submitted by the user such as account details but also behavioural data
generated through user activity, while it excludes data that might be
generated by the controller after further processing, such as user
profiles.

Data should be provided in a format that is abstracted away from the
specific technical implementation of the controller’s systems, since the
aim is that the data can easily be repurposed by the user; the goal is
interoperability. Case law has established that PDF (Portable Document
Format) files do not meet this goal.^59^ The ability to export data only
in small chunks (for example, one email at a time) is also not
sufficient.^60^ Data should be provided within 30 days, although an
extension to three months is possible, as long as the controller
explains to the data subject why it needs more time. If there is too
much data to transmit digitally within a reasonable time frame,
controllers should consider alternative (physical) media.

## Article 40 of the Digital Services Act 

The DSA^61^ regulates online services to try to make them safer and more
transparent.^62^ Article 40 of the DSA tries to support this by
requiring ‘very large online platforms’ (for example, Facebook, Amazon,
and Booking.com) and ‘very large search engines’ (for example, Google
and Bing) to give researchers access to data which can be used to study
systemic risks that might be created because of the service, as well as
data that can be used to assess whether the risk mitigation measures
that the services themselves have implemented are adequate and
efficient.

There is still a lot of uncertainty about who qualifies for access and
what data can be requested (see chapter 10 for a more detailed
discussion). Much of the Article confers rights to so-called vetted
researchers, which are individuals who meet a number of requirements
(that is, affiliated with a research organization and independent of
commercial interests) and can fulfil certain operational obligations
(for example, capable of providing the necessary security and
confidentiality for the data and make results publicly available for
free). Their application for this status also needs to justify the
duration for which they need access to the data and explain how their
research contributes to systemic risk assessment and mitigation
monitoring. However, paragraph 12 of the Article also opens up access
rights to individuals who meet those conditions but are not affiliated
with a research organization and do not commit to making their results
publicly available for free.

If an individual is officially designated as a ‘vetted researcher’ by
the Digital Services Coordinator (DSC, the main national authority), the
individual can ask the DSC to pass on the data access request to the
digital service on their behalf. The requested data should be received
‘within a reasonable period’ and be provided through ‘appropriate
interfaces’ (for example, online databases and application programming
interfaces). Those individuals who qualify for access as per paragraph
12 can request data directly without going through the DSC, which they
should receive ‘without undue delay’ and (if possible) in ‘real time’.

## Article 6(9) of the Digital Markets Act 

The DMA^63^ tries to address characteristics of digital businesses that
result in a lack of contestability (that is, monopolies), such as
network effects, multiside markets, vertical integration, and extreme
economies of scale. It targets gatekeepers, platforms that, over the
last three years, have had a large impact on the EU’s internal market
and had at least 45 million monthly active end users and 10,000 yearly
active business users.

Article 6(9) of the DMA describes data sharing obligations for the ‘core
platform services’ of those gatekeepers that are supposed to stimulate
contestability, by making it easier to leave a platform or have it
inter-operate with another service. The Article specifies that
gatekeepers need to provide end users (or third parties authorized by
them) the option to port the data they have provided to the gatekeeper
or data that is generated by their use of the service. That data should
be provided in a format that can be used ‘immediately and
effectively’^64^ (that is, a commonly used format in a manageable size),
and gatekeepers also need to implement ‘appropriate and high-quality
technical measures, such as application programming interfaces’^65^ that
make it possible to access this data ‘continuously and in real time’.

## Articles 3 and 4 of the Data Act

EU policies related to data have longstanding tensions between, on the
one hand, wanting to protect fundamental rights while, on the other,
furthering economic growth by removing barriers for companies.^66^ If
the GDPR represents regulation that primarily tries to achieve the
first, the Data Act could be seen as the outcome of the second interest,
as it sets out to ‘maximise the value of data in the economy and
society’.^67^ It sets down rules to (among other things) make product
and related service data available to the users of those products.

Article 3 of the Data Act confers the right to end users to have access
to any product data, product-related service data, and metadata
necessary to understand and use that data. This data should be made
available by default, easily, in a comprehensive, structured, commonly
used, and machine-readable format. Users should also, before purchasing
a product or a related service, be given information about the type,
format, estimated volume, and frequency of data the product is capable
of generating, whether it stores this on a device or a remote server and
whether the data holders expect to use the data themselves for specific
purposes and/or share it with third parties.

Article 4 of the Data Act adds specifications to the data rights
described in Article 3 – for example, that data access can be restricted
if it undermines the security requirements of the product, that the data
holder should notify the competent authorities if it restricts data
access, that users can lodge complaints in this case, and that data
holders cannot make exercising this right unduly difficult. Arguably,
the most important restriction relates to the protection of trade
secrets, which is a valid reason to not share data if the user does not
take measures to ensure their confidentiality. However, such decisions
should be substantiated and the competent authorities should be notified
of it, which are burdens placed on data holders to make sure it does not
restrict the user’s data rights for their own gain.

\*\*\*

The access rights given to users across these different EU regulations
overlap and expand on each other, sometimes explicitly (as in the case
of the DMA expanding GDPR rights). They target different spheres of
contemporary, digitally mediated life (for example, online services,
connected products, and data-hungry governments) and try to rebalance
the distribution of power through different approaches (for example,
improving fundamental rights, consumer protection, and fair
competition). Whether these rights are enough to address remains to be
seen. Only the access right to personal data has been in place for long
enough to have had an impact, and preliminary signals indicate low
levels of compliance^68^ and no increase in people’s feelings of control
over their personal data.^69^

# Teaching Data Literacy Using Access Rights 

The EU’s imaginary of access rights and the goals of teachers advocating
for data literacy overlap: both want to empower citizens against the
negative externalities of informational capitalism.^70^ The EU sees
access to data as a precondition for good citizenship in a digital
society – for example, by making it possible for people to oversee the
fair processing of their personal data or by being able to move their
data to other services rather than being locked in. Data literacy
advocates think that population-wide competences to understand the
generation and analysis of data is necessary to mitigate digital
inequities that arise between those who can work with data and those who
cannot, which is becoming more pressing in societies where data-based
knowledge claims about the world carry more weight.^71^ Given this
overlap in mission, how can access rights contribute to the development
of data literacy in practice?

Over the past three years we have used access rights as part of our
teaching in the Department of Information Studies at Aarhus University,
Denmark. The department positions itself within the broader humanities
faculty as the department for critical reflection on technology and
society through theory, empirical research, and the construction of
digital artefacts (for example, prototypes, software, and art). Across
the different degree programmes, students are broadly taught science and
technology studies (STS) theory, human–computer interaction (HCI) design
approaches, digital innovation, programming, and qualitative methods. We
exercised the right to access under Article 15 of the GDPR (since the
other rights were not yet in effect) in three different syllabi: Data
Studies, Datafication of Society, and Digital Living. The Data
Studies^72^ course is a second-year bachelor’s degree course that aims
to teach the students how to think critically about the production and
use of data through theoretical frameworks and hands-on data processing
(for example, querying application programming interfaces (APIs) and
using machine learning \[ML\]). The Datafication of Society^73^ course
is a third-year bachelor’s degree course that tries to place the
restructuring of societies around data in the context of larger
historical and sociological trends. The Digital Living master’s
programme^74^ is an interdisciplinary degree that bridges social theory,
business, management, and computer science and includes a one-week
cross-course module on surveillance capitalism to show how to combine
the perspectives from different disciplines. In all these courses, we
have used access rights as part of our teaching to help the students
understand what data is collected, how technologies work, or how data is
part of organizational practices.

Based on these experiences, we see four ways in which access rights have
contributed to the development of students’ digital literacy: (*a*) the
experience of trying to exercise access rights reveal the power
relations between technology companies and nation states; (*b*) the data
the students received helps them confront their internalized dataism;
(*c*) the meta-information about whom companies receive data from and
share it with can be used to trace data flows; and (*d*) comparing
outcomes of different data processing methods shows the epistemological
impact that mediating artefacts have on the realities and knowledge that
are (co-)created by technology companies. We elaborate on the goals,
exercises, and learning outcomes of each of these four in the following
sections. Although not all exercises were used in all courses, we do
present them here as a coherent trajectory in a suggested order where,
for example, data received in one exercise can be reused in the next or
previous learning outcomes make the next goal easier to understand.

## Power Relations of Informational Capitalism

### Goal

This exercise seeks to teach students about their digital rights and to
demonstrate how current power relations between companies and nation
states impact the effectiveness of those rights.

### Exercise

Before the session, students are asked to read about their data rights
on the website of their national data protection authority.^75^ During
the session, we first discuss whether the students know about these
rights, what they think their purpose is, and if they have ever used
them. We then collectively go through the process of exercising their
right to access. We brainstorm about possible data controllers they
could send a request to, making sure to go beyond just the obvious Big
Tech companies so the students stretch their understanding of the extent
of datafication. Once we have compiled a list, each student creates and
sends a request to a controller of their choice. We use the My Data Done
Right (mydatadoneright.eu) tool to generate the request text, which is a
user-friendly interactive form created by the Dutch privacy organization
Bits of Freedom, but students send the request from their own email
accounts. We tell them that they can consult us if they need help in the
follow-up process with the company after sending the request. One month
later, there is a follow-up poll and discussion where we ask the
students how many of them received a response, what that response
included, whether they feel empowered, what surprised them, and whether
they changed their perception on access rights, technology companies,
and the law.

### Learning Outcomes

Data access rights are supposed to empower individuals, but a
precondition for this is that people know about their rights and
actively use them. Incorporating access rights into the curriculum is a
concrete and straightforward way to teach larger cohorts of citizens
about their rights, which they will hopefully continue to use outside
the classroom and throughout their lifetime.

Exercising the access rights, at least right now, also demonstrates to
students that laws on paper are different from laws in practice.
Research shows that many organizations lack awareness and understanding
of the rights,^76^ do not respond within legal timeframes,^77^ and (when
they do) often fail to provide all the required information.^78^ These
results are confirmed by our classroom experiences, where (in the latest
iteration of the Data Studies course in 2022 with approximately 90
students) only around 7 per cent received a complete response (67 per
cent received a response, and of those only 11 per cent received all
necessary information: confirmation of data processing,
meta-information, and a copy of the data). In Denmark, a country with a
strong cultural belief in the rule of law^79^ and trust in
institutions,^80^ students are often confused by this experience.
Especially those who send data subject access requests to Big Tech or
large social medial companies (for example, Meta, Google, Snapchat, and
TikTok) are shocked by how they are ignored. These experiences have
proven to be a helpful starting point to critically discuss power
relations between sovereign nation states and wealthy technology
companies and the effectiveness of law as an instrument to redistribute
such power. It raises questions such as following: Why are companies
making it difficult to exercise these rights? Why are authorities not
able to make them comply with the laws? What is the political philosophy
behind individual rights as a remedy to informational power?

## Confronting Dataism

### Goal

This exercise seeks to uncover and question students’ unspoken
assumptions about the quality and objectivity of company’s data
processing and reveal how much data is collected about them.

### Exercise

Before the class, students are asked to get copies of their personal
data from any digital platforms they use. Ideally, this would be based
on copies they receive through their access rights, but that process is
currently too slow and unreliable to feasibly build exercises around.
Instead, we ask students to use the ‘download your data’ options that
many platforms have started to offer in response to access rights,
although we make sure to emphasize that this is not the same as
exercising their access right and is likely an incomplete dataset. In
the class, the students are asked to review those files, by paying
attention to the quantity of the data collected, the quality of that
data, and the inferences and classifications made by the company about
them. We suggest they can do this in pairs if they feel comfortable
revealing personal data, so they can compare with other students how
they are perceived differently by the same company. We also provide some
scaffold code (in the form of Jupyter Notebooks) that helps them produce
aggregates such as counts, averages, distributions, or visualizations.
We conclude with a discussion about what they found, what surprised
them, and why they were surprised to help surface their unconscious
assumptions.

### Learning Outcomes

The students we meet are often socialized into the belief that data
represents objective, value-neutral measures of the world and that using
it, especially in large quantities, will naturally lead to better
outcomes, predictions, or products.^81^ Confronting this *dataism*,^82^
this ideology of the unreasonable effectiveness of data facts, is an
important component of the critical approaches to data literacy.
However, because this is an ideology, we often encounter some conscious
and unconscious resistance from the students to this critique. As Jose
van Dijck explains, dataism is not just the belief that data could
capture the world ‘as is’ but also the trust in the institutions and
companies that collect, clean, and analyse this data. In our experience,
simply mentioning that the full complexity of the world cannot be
captured by quantitative data is not that hard for the students to
accept. The more difficult barrier to overcome is their assumption that
data handlers also know this, have processes in place to overcome these
limitations, and surely only draw justifiable conclusions from the data.
Irrationality, irresponsibility, pragmatism, and the primacy of
profit-seeking are given less weight in their imagination of
datafication.

Using access rights has been an effective method to confront the
students with the volume of collected data and their assumptions about
the quality of its processing. Other scholars have already suggested
using ‘real data’ or data the students have a connection to because it
is more engaging,^83^ but personal data specifically makes it possible
for students to perceive the epistemic distance between their experience
and the data double^84^ a company has constructed of them. Because the
students know themselves, they are often surprised about the amount of
data that is being associated with them and incredulous about the (often
poor-quality) assumptions, predictions, and categorizations of companies
they might hold in high regard. It is hard to drive these points home
without personal data: to evaluate whether the volume of data is a lot
or not is something that rests on the contrast between the lived
experience of using digital services and the data traces they never knew
about or reasonably expected. And to evaluate the quality of that data
requires that the students know what the underlying reality that data is
supposedly capturing.

Justifications for existing technologies and battles over visions of the
digital future are often fought through discourse and symbolism.^85^
Confronting the students with the actual data that lies behind the
imaginaries of technology companies – datasets as a ‘higher form of
intelligence’^86^ – helps dispel some of the myths that are created and
hopefully inoculates the students against future hype cycles.

## Tracking Data Flows 

### Goal

This exercise seeks to show how data flows through a network of many
different actors and how it gets reshaped at each of those steps.

### Exercise

Students are asked to pick a core digital service in their life and
trace all the other parties that this organization receives personal
data from and shares personal data with. This metadata should be
included in responses to an access request under Article 15 of the GDPR,
but since transparency obligations in EU regulation (GDPR, DA, DSA)
require that such information is also available more publicly, a
fall-back option is to look at privacy policies, terms and conditions,
consent banners, and any other information that describes the data
processing of that organization (keeping in mind that these are not
entirely equivalent: access requests should include the exact identity
of the recipients of personal data,^87^ whereas in privacy policies
‘categories of recipients’ are enough). Each party can be mapped based
on various characteristics (for example, geographical location, type of
service, level in the software stack, and annual turnover). Follow-up
access requests or investigations into those parties should then provide
an insight into what data has been shared with them from the original
service, how it is augmented and transformed, what the data is used for,
and an additional list of data-sharing partners that could be the seed
for the next wave of access requests.

### Learning Outcomes

Infrastructural perspectives on data often emphasize its social and
material entanglements:^88^ the physicality of the internet with its
undersea cables and landing sites, the cultural and political incentives
that inform categorizations,^89^ the public infrastructural
responsibilities assumed by private platforms,^90^ and the individual
and organizational subjectivities that shape processes of data
cleaning.^91^ This insight is crucial to solidify the understanding that
‘raw data is an oxymoron’^92^ and that data is always being reprocessed
and repurposed as it flows through a complex network of actors. Making
those networks explicit – what Geoffrey C. Bowker and Susan L. Star call
‘infrastructural inversion’^93^ – is the first step in locating power
and allocating responsibilities to certain players.

Data access rights include access to metadata, such as where the data
comes from (if not provided by the person themself), how long it is
stored for, who else the data is shared with, the location of those
third parties, and their trading names and contact details. This
information can then be used to make consecutive access requests to data
held by other parties, allowing students to map out all the different
players in the ecosystem, see how the data gets transformed and
augmented at each step, what it is used for, and which other
infrastructures it touches. What it demonstrates is that data should
always also be thought of through the lens of ecosystems or networks,
since its shape, its assumed value, and its impact are not inherent to
the data itself but instead emerge because of how it flows through a
particular chain of players. The same data might be harmless in
isolation or when it stays in the hands of a single player (for example,
an account on a period-tracking app), but becomes dangerous when
combined with other data (for instance, location data) or when shared
with other actors (for example, anti-abortion organizations).

## Epistemological Impact of Mediation

### Goal

This exercise seeks to explain how knowledge is shaped by the digital
artefacts that mediate its production and that claims of truth are
always based on a particular philosophical position.

### Exercise

Following a lecture on the fundamentals of ML, students are provided
with interactive code notebooks (Jupyter Notebooks) which explain and
demonstrate how ML models for natural language processing and image
processing work. The first notebook focuses on sentiment analysis – the
prediction of emotional value in text – comparing the VADER and TextBlob
models.^94^ The second notebook focuses on image classification –
assigning a label to an image – and compares the EfficientNetV2 and
ResNeXt models.^95^ Students are asked to feed the models with their own
input data (for example, text messages they sent or images they posted
on social media) and reflect on the output they get. As part of their
reflection, they are asked to find documentation about the people behind
the model, what data it was trained on, what it was created for, where
it is being deployed, what claims and decisions are made based on it,
and so on.

### Learning Outcomes

Mediation theoretical perspectives on technology such as
postphenomenology and actor–network theory highlight how our perceptions
and actions are coconstituted by the artefacts that sit between
ourselves and the world (often blurring ontological separations between
subject and object). In the case of digital technologies, mediating
artefacts such as algorithms and models help generate a particular view
of the world and structure our actions in finite ways. These are quite
abstract notions about the nature of being and knowing, but exercises
comparing multiple mediating artefacts demonstrate quite concretely how
they generate different outputs. Using very specific technologies also
makes it possible for the students to trace design decisions that were
made (for example, training approaches and parameters) and the other
artefacts that are involved (for example, datasets and platforms).
Learning how to do this kind of methodological deconstruction is crucial
both to evaluate the quality of a knowledge claim made by others and for
the students’ ability to be intentional and transparent about any
knowledge they themselves might create using digital tools.^96^ At a
higher level, a concrete understanding of the epistemological impact of
mediation opens up discussions about whose views are being represented
and what kind of values are expressed by design decisions.^97^ If
real-time and continuous data access is available, a more constructive
approach to data literacy could encourage students to build alternative
software (for example, apps, websites, and visualization pipelines) that
processes their data differently – what Mireille Hildebrandt calls
‘agonistic machine learning’^98^ or Henrik Korsgaard, Clemens N.
Klokmose, and Susanne Bødker call ‘computational alternatives’^99^ –
and, through them, give shape to the digital worlds that they would like
to live in.

\*\*\*

What data literacy means and how it should be taught is not a settled
question yet, although broadly speaking it includes competences in
working with data and being able to evaluate its value and impact. In
our experience, the four exercises presented here help students (*a*)
become aware of the politics of informational capitalism, (*b*) confront
their internalized dataism, (*c*) track how data flows through global
networks, and (*d*) realize how mediating artefacts impact the knowledge
that can be derived from data. For educators interested in the
instrumental aspect of data literacy, these exercises provide
interesting data for students to process and analyse; and for those
arguing for a critical perspective on data literacy, these exercises can
be used for non-technical students to reflect on and discuss the larger
political structures that data is part of.

# Limitations of Using Access Rights in Education

Based on our personal experiences and reflections described earlier,
access rights can help university students develop data literacy.
However, exercising them does not necessarily teach everything that
researchers have indicated is important for developing this competence,
and it should sit next to other approaches. The legal design of access
rights and the way they are interpreted by technology companies place
some limitations on how they can be used as part of a pedagogical
strategy, which we highlight here. The limitations related to the DSA,
the DMA, and the Data Act are speculative, since these have not gone
into effect yet at the time of writing this and we have not had the
chance to include them in our teaching.

GDPR access rights are quite easy to exercise in practice: they can be
sent to any reasonable contact address, they require no additional
technical expertise or financial resources, and there are plenty of text
templates available. However, their focus on personal data makes them
less suited for creating transparency around the processing methods or
technical infrastructure of the organization, since they only reveal the
digital double of an individual rather than the various algorithms,
models, or databases this data is used in (for example, a person’s
coordinates within a multidimensional recommender model). Devoid of the
context, personal data can become rather arbitrary since decisions and
classifications derive meaning from their relative position to other
numbers (for example, the Instagram device setting ‘face\_filter’: ‘14’
or Spotify’s ad category inference ‘dfp\_expiration\_test\_ krishna’
does not reveal much). It is possible to get a sense of the larger
system by having students compare these values between them, but the
personal nature of the data makes such collective aggregation a
sensitive exercise. Poor compliance rates and one-month reply durations
also make it difficult to build consecutive exercises around them within
a three-month semester. At the same time, these rights have also
stimulated ‘data download’ options and public reporting of similar
information that provide feasible fallback options.

Access rights under the DSA provide interesting transparency about
larger systems because they go beyond personal data and are instead
anchored in the goal to detect systemic risks and evaluate risk
management strategies, which makes the data that falls inside that scope
much more variable. However, these access rights only apply to a handful
of digital services (19 at the time of writing) and do not require
sharing information about how the data flows beyond the borders of the
organization, making it less suited for tracing networks. The access
rights are also more restricted because they require an individual to be
approved as a ‘vetted researcher’ by the supervisory authority, because
the access is limited to the time it takes to do the research and
because the researcher needs to provide ‘necessary security and
confidentiality’ measures (which likely do not include sharing the data
with over a hundred students). Rather than sharing their access with
students directly, researcher-teachers could instead generate synthetic
datasets or replicate processing pipelines that are suitable for
teaching (that is, without reidentification and model inversion risks),
either for their own courses or to share with other educators. Paragraph
12 of Article 40 of the DSA, which requires platforms to give real-time
data access to individuals not vetted by the DSC, might also stimulate
platforms to provide student accessible APIs or sandbox environments as
a gesture towards compliance, similar to how personal data download
options appeared in response to GDPR access rights.

There is considerable overlap in the access rights established by the
DMA and the Data Act. Both provide access to data provided or generated
by the end user, continuous and in real time (although the Data Act adds
more caveats to the access method: ‘where relevant and appropriate’,
‘where applicable’). The main difference between the two is in who it
applies to. The DMA rights are, like the DSA, applicable to only a small
number of ‘gatekeepers’ (seven at the time of writing) and only about
their ‘core platform services’. The Data Act applies to manufacturers of
connected products and providers of related data-generating services on
the EU market. Another difference is that the Data Act also requires
organizations to provide considerable transparency about what data is
generated, how much and in what level of granularity, and who it is
shared with. Both the DMA and the Data Act make it possible for users to
request that their data is shared directly with third parties, which
could include services that analyse and visualize that data in ways that
contribute to the development of a student’s data literacy. The
practical value of these rights for education, however, will depend
almost entirely on how they are implemented. A generous interpretation
of these access rights would mean a variety of parameterized APIs that
output actual data values without any rate limits, while a more
restricted interpretation would be a sandboxed environment where
computations can be ran ‘in situ’ but which keeps individual data points
obfuscated inside the platform.^100^

# Conclusion 

New EU regulations are poised to expand access rights to how digital
companies collect and process data, doubling down on the EU’s governance
strategy to combine formal oversight with a human rights approach. How
access rights contribute to structural change in the power relations of
digitalized societies is not entirely clear. In this chapter, we
discussed how access rights can be used in the context of education,
connecting to ongoing pedagogical efforts across various disciplines
around the idea of data literacy as ‘an important part of a strategy in
democratic societies to come to terms with living in a digital
world’.^101^ Based on our experiences teaching at a Danish university
over the last three years, we suggest that access rights can contribute
to the development of students’ data literacy in four ways: (*a*) the
experience of trying to exercise access rights and the poor compliance
rates reveal the political tension and *power relations between
technology companies and nation states*; (*b*) the quality and quantity
of the data revealed through data access helps *students confront their
internalized dataism*; (*c*) the meta-information about whom companies
receive data from and share it with can be used to *trace data flows*
and demonstrate the importance of a *network and infrastructure
perspective* on data; and (*d*) comparing outcomes of different data
processing methods shows the *epistemological impact* that *mediating
artefacts* have on the realities and knowledge that is (co-)created by
technology companies.

The use of existing and upcoming access rights for education also has
limitations, primarily the history of dismal compliance rates (because
of either lack of competences or active subversion of the law). Other
limitations include constraints placed on who those access rights are
for (that is, vetted researchers), who it applies to (for example, very
large online services or gatekeepers), what kind of technical
competences are required to exercise or analyse them (for instance,
understanding of structured data formats or making API calls), or the
ambiguous language of the access right obligations that give
considerable interpretative freedom to organizations controlling data.
While the actual exercise of access rights might not necessarily provide
the organizational transparency and individual empowerment that
regulators imagine, we expect that they will create more opportunities
for opening up the black boxes of technologies broadly. We have seen
something similar with current access rights, where even lip service to
these obligations (for example, data download buttons and updated
privacy policies) have proven to provide accessible opportunities for
our in-class activities. In other words, we do not need every single
access right to be respected for them to create new pedagogical
opportunities that could contribute to the development of students’ data
literacy as long as data and insights based on access are shared
publicly or between educators.

Future work looking at the intersection of access rights and higher
education could include more formal evaluations of specific exercises
and learning outcomes for students in order to check whether and to what
extent they contribute to data literacy or empowerment more generally.
However, this would also require more concretization of the concept of
data literacy (what exactly are the most important components and
competencies) and a discussion of acceptable evaluation strategies,
since the disciplines involved in these efforts span across
epistemological traditions and might not be convinced by the same kind
of ‘evidence’. One elephant in the room that should also be discussed is
the fact that centring data literacy as a response to the negative
externalities of datafication is a political choice^102^ without clear
evidence that it can achieve those purposes,^103^ and that this choice
naturally takes away resources from approaches that do not look at these
issues through the lenses of individual resistance, data, or digital
technology. Collective approaches or established theoretical traditions
of power and political economy (and thus perhaps less glittery than data
literacy) should also be considered as legitimate foundations for a
pedagogy of the oppressed in digital societies.

# Notes

1.  EU General Data Protection Regulation (GDPR): Regulation (EU)
    2016/679 of the European Parliament and of the Council of 27 April
    2016 on the protection of natural persons with regard to the
    processing of personal data and on the free movement of such data,
    and repealing Directive 95/46/ EC (General Data Protection
    Regulation), OJ 2016 L 119/1.

2.  J. Jaursch, ‘Transcript for the Background Discussion “New EU-rules
    for Big Tech: How to Improve the Digital Services Act”’, Stiftung
    Neue Verantwortung, 14 September 2021,
    https://www.stiftung-nv.de/en/
    publication/transcript-background-discussion-new-eu-rules-big-tech-howimprove-digital-services-act
    (accessed 25 August 2023).

3.  EU Digital Markets Act (DMA): Regulation (EU) 2022/1925 of the
    European Parliament and of the Council of 14 September 2022 on
    contestable and fair markets in the digital sector and amending
    Directives (EU) 2019/1937 and (EU) 2020/1828 (Digital Markets Act),
    OJ 2022 L 265.

4.  EU Data Act (DA): Regulation (EU) 2023/2854 of the European
    Parliament and of the Council of 13 December 2023 on harmonised
    rules on fair access to and use of data and amending Regulation (EU)
    2017/2394 and Directive (EU) 2020/1828 (Data Act), OJ 2023.

5.  O. Solon, ‘How Much Data Did Facebook Have on One Man? 1,200 Pages
    of Data in 57 Categories’, *Wired*, 28 December 2012,
    https://www.wired.

> co.uk/arcle/privacy-versus-facebook (accessed 25 August 2023).

1.  *Uber v. Drivers* \[2023\] ECLI:NL:GHAMS:2023:796 Court of Appeal.

2.  As one of the few examples, see S. Yates and E. Carmi*, Developing
    Citizens Data Literacy: A Short Guide* (London: Nuffield Foundation,
    2022).

3.  R. Collins, *The Credential Society: An Historical Sociology of
    Education and Stratification* (Columbia University Press, 2019).

4.  In many European countries, for example, higher education
    institutions were used in the last 50 years as an important ‘engine’
    to bring about the structural evolution of the country into a
    ‘knowledge society’, leading to universities dramatically increasing
    their student populations and changing their management style to be
    more like a commercial entity. Jussi Välimaa and David Hoffman,
    ‘Knowledge Society Discourse and Higher Education’, *Higher
    Education* 265, no. 56 (2008): 265–285.

5.  P. Freire, *Pedagogy of the Oppressed*, 30th anniversary ed.
    (Continuum, 2000 \[1968\]).

6.  bell hooks, *Teaching Critical Thinking: Practical Wisdom*
    (Routledge, 2010).

7.  C. Fiesler, N. Garrett, and N, Beard, ‘What Do We Teach When We
    Teach Tech Ethics? A Syllabi Analysis’, in *Proceedings of the 51st
    ACM Technical Symposium on Computer Science Education* (Association
    for Computing Machinery, 2020), DOI:
    https://dl.acm.org/doi/10.1145/3328778.3366825.

8.  D. Carrington and S. K. Kim, ‘Teaching Software Design with Open
    Source Software’, 33rd Annual Frontiers in Education, FIE
    2003, 2003.

9.  H. Kennedy and G. Moss, ‘Known or Knowing Publics? Social Media Data
    Mining and the Question of Public Agency’, *Big Data and Society* 1,
    no. 2 (2015), DOI: https://doi.org/10.1177/20539517156111.

10. L. Pangrazio and N. Selwyn, ‘“Personal Data Literacies”: A Critical
    Literacies Approach to Enhancing Understandings of Personal Digital
    Data’, *New Media and Society* 419, no. 21 (2019): 419–437.

11. danah boyd and Kate Crawford, ‘Critical Questions for Big Data:
    Provocations for a Cultural, Technological, and Scholarly
    Phenomenon’, *Information, Communication and Society* 662, no. 15
    (2012): 662–679.

12. Huw C. Davies, ‘Rescuing Data Literacy from Dataism’, in *Data
    Justice and the Right to the City*, ed. Morgan Currie, Jeremy Knox,
    and Callum McGregor, 146–164 (Edinburgh University Press, 2022).

13. Chantel Ridsdale, James Rothwell, Mike Smit, Hossam Ali-Hassan,
    Michael Bliemel, Dean Irvine, Daniel Kelley, Stan Matwin, and Brad
    Wuetherick, ‘Strategies and Best Practices for Data Literacy
    Education: Knowledge Synthesis Report’, Dalhouse University, 2015.

14. J. E. Raffaghelli and B. Stewart, ‘Centering Complexity in
    “Educators” Data Literacy’ to Support Future Practices in Faculty
    Development: A Systematic Review of the Literature’, *Teaching in
    Higher Education* 435, no. 25 (2020): 435–455.

15. T. Koltay, ‘Data Literacy for Researchers and Data Librarians’,
    *Journal of Librarianship and Information Science* 3, no. 49 (2017):
    3–14.

16. Elinor Carmi, Simeon J. Yates, Eleanor Lockley, and Alicja
    Pawluczuk, ‘Data Citizenship: Rethinking Data Literacy in the Age of
    Disinformation, Misinformation, and Malinformation’, *Internet
    Policy Review* 9, https:// policyreview.info/node/1481 (accessed 25
    August 2023); D. Crusoe, ‘Data Literacy Defined Pro Populo: To Read
    This Article, Please Provide a Little Information’, *Journal of
    Community Informatics* 12 (2016), https://
    openjournals.uwaterloo.ca/index.php/JoCI/article/view/3276 (accessed
    25 August 2023).

17. C. D’Ignazio and R. Bhargava, ‘Approaches to Building Big Data
    Literacy’, Proceedings of the Bloomberg Data for Good Exchange
    Conference, New York, NY, 2015.

18. L. Dogruel, P. Masur, and S. Joeckel, ‘Development and Validation of
    an Algorithm Literacy Scale for Internet Users’, *Communication
    Methods and Measures* 115, no. 16 (2022).

19. J. Gray, C. Gerlitz, and L. Bounegru, ‘Data Infrastructure
    Literacy’, *Big Data and Society* 1, no. 5 (2018): 115–133.

20. C. D’Ignazio and R. Bhargava, ‘Cultivating a Data Mindset in the
    Arts and Humanities | Public’, *Public* 4, no. 2 (2018),
    https://public.imaginingamerica.
    org/blog/article/cultivating-a-data-mindset-in-the-arts-and-humanities
    (accessed 25 August 2023).

21. Crusoe, ‘Data Literacy Defined Pro Populo’.

22. D’Ignazio and Bhargava, ‘Approaches to Building Big Data Literacy’.

23. Gray, Gerlitz, and Bounegru, ‘Data Infrastructure Literacy’; I.
    Sander, ‘What Is Critical Big Data Literacy and How Can It Be
    Implemented?’ *Internet Policy Review* 9, no. 2 (2020),
    https://policyreview.info/node/1479 (accessed 25 August 2023).

24. D’Ignazio and Bhargava, ‘Approaches to Building Big Data Literacy’.

25. T. Bucher, ‘The Algorithmic Imaginary: Exploring the Ordinary
    Affects of Facebook Algorithms’, *Information, Communication and
    Society* 30, no. 20 (2017): 30–44.

26. Crusoe, ‘Data Literacy Defined Pro Populo’.

27. T. Philip, M. Olivares-Pasillas, and J. Rocha, ‘Becoming Racially
    Literate about Data and Data-Literate about Race: Data
    Visualizations in the Classroom as a Site of Racial-Ideological
    Micro-Contestations’, *Cognition and Instruction* 361, no. 34
    (2016): 361–388.

28. Gray, Gerlitz, and Bounegru, ‘Data Infrastructure Literacy’.

29. D’Ignazio and Bhargava, ‘Cultivating a Data Mindset in the Arts and
    Humanities’.

30. Pangrazio and Selwyn, ‘“Personal Data Literacies”’.

31. Karin van Es, Nicholás L. Coombs, and Thomas Boeschoten, ‘Towards a
    Reflexive Digital Data Analysis’, in *The Datafied Society: Studying
    Culture Through Data*, ed. Karin van Es and Mirko T Schäfer, 171–180
    (Amsterdam University Press, 2017).

32. Sander, ‘What Is Critical Big Data Literacy and How Can It Be
    Implemented?’

33. Ying Cui, Fu Chen, Alina Lutsyk, Jacqueline P. Leighton, and Maria
    Cutumisu, ‘Data Literacy Assessments: A Systematic Literature
    Review’, *Assessment in Education: Principles, Policy and Practice*
    76, no. 30 (2023): 76–96.

34. Bahareh Ghodoosi, Tracey West, Qinyi Li, Geraldine Torrisi-Steele,
    and Sharmistha Dey, ‘A Systematic Literature Review of Data Literacy
    Education’, *Journal of Business and Finance Librarianship* 112, no.
    28 (2023): 112–127.

35. Ridsdale, Rothwell, Smit, Ali-Hassan, Bliemel, Irvine, Kelley,
    Matwin, and Wuetherick, ‘Strategies and Best Practices for Data
    Literacy Education’; Cui, Chen, Lutsyk, Leighton, and Cutumisu,
    ‘Data Literacy Assessments’.

36. Ridsdale, Rothwell, Smit, Ali-Hassan, Bliemel, Irvine, Kelley,
    Matwin, and Wuetherick, ‘Strategies and Best Practices for Data
    Literacy Education’; Cui, Chen, Lutsyk, Leighton, and Cutumisu,
    ‘Data Literacy Assessments’.

37. Ridsdale, Rothwell, Smit, Ali-Hassan, Bliemel, Irvine, Kelley,
    Matwin, and Wuetherick, ‘Strategies and Best Practices for Data
    Literacy Education’; Cui, Chen, Lutsyk, Leighton, and Cutumisu,
    ‘Data Literacy Assessments’.

38. D’Ignazio and Bhargava, ‘Cultivating a Data Mindset’.

39. S. Vakil, A. Reith, and N. A. Melo, ‘Jamming Power: Youth Agency and
    Community-Driven Science in a Critical Technology Learning Program’,
    *Journal of Research in Science Teaching* 60, no. 8 (2023), DOI:
    https:// onlinelibrary.wiley.com/doi/abs/10.1002/tea.21843.

40. I. Sander, ‘Critical Big Data Literacy Tools: Engaging Citizens and
    Promoting Empowered Internet Usage’, *Data and Policy* 1, no. 2
    (2020): e5.

41. Rahul Bhargava, Amanda Brea, Victoria Palacin, Laura Perovich, and
    Jesse Hinson, ‘Data Theatre as an Entry Point to Data Literacy’,
    *Educational Technology and Society* 93, no. 25 (2022): 93–108.

42. D’Ignazio and Bhargava, ‘Cultivating a Data Mindset’.

43. D’Ignazio and Bhargava, ‘Cultivating a Data Mindset’.

44. L. Poirier, ‘Ethnographies of Datasets: Teaching Critical Data
    Analysis through R Notebooks’, *Journal of Interactive Technology
    and Pedagogy* (2020),
    https://jitp.commons.gc.cuny.edu/ethnographies-of-datasets-teachingcritical-data-analysis-through-r-notebooks
    (accessed 25 August 2023).

45. Starting with the Data Protection Act of the German state Hesse
    (Hessisches Datenschutzgesetz vom 7 oktober 1970 GVBl II 300-10,
    published at Wiesbaden, 12 October 1970, in *Gesetz-und
    Verordnungsblatt für das Land Hessen* \[Laws and Regulations
    Journal\], part 1, no. 41). For in-depth overviews of the history
    and emergence of data protection and data rights, see, for
    example, G. G. Fuster, *The Emergence of Personal Data Protection as
    a Fundamental Right of the EU* (Springer, 2014) and J. Ausloos, *The
    Right to Erasure in EU Data Protection Law: From Individual Rights
    to Effective Protection* (Oxford University Press, 2020).

46. EU Directive 95/46: Directive 95/46/EC of the European Parliament
    and of the Council of 24 October 1995 on the protection of
    individuals with regard to the processing of personal data and on
    the free movement of such data, OJ 1995 L 281/31.

47. GDPR, OJ 2016 L 119/1, Article 15.

48. GDPR, OJ 2016 L 119/1, Article 15.

49. European Commission, ‘Commission Staff Working Paper Impact
    Assessment Accompanying the Document Regulation of the European
    Parliament and of the Council on the Protection of Individuals with
    Regard to the Processing of Personal Data and on the Free Movement
    of Such Data (General Data Protection Regulation) and Directive of
    the European Parliament and of the Council on the Protection of
    Individuals with Regard to the Processing of Personal Data by
    Competent Authorities for the Purposes of Prevention, Investigation,
    Detection or Prosecution of Criminal Offences or the Execution of
    Criminal Penalties, and the Free Movement of Such Data’, 2012,
    https://
    eur-lex.europa.eu/legal-content/EN/TXT/?uri=CELEX%3A52012SC0072
    (accessed 1 December 2024).

50. Such as the French *Loi Informatique et Libertés* of 1978 (*Loi n°
    78–17 du 6 janvier 1978 relative à l’informatique, aux fichiers et
    aux libertés*).

51. Such as the Data Protection Directive, 1995. OJ 1995 L 281/31.

52. European Data Protection Board, ‘Guidelines 01/2022 on Data Subject
    Rights: Right of Access’, 2022, https://www.edpb.europa.eu/system/
    files/2023-04/edpb\_guidelines\_202201\_data\_subject\_rights\_access\_v2\_
    en.pdf (accessed 1 December 2024).

53. European Data Protection Board, ‘Guidelines on the Right to Data
    Portability under Regulation 2016/679, WP242 rev.01’, 2017,
    https://www.edpb.
    europa.eu/our-work-tools/our-documents/guidelines/guidelines-right-dataportability-under-regulation-2016679\_en
    (accessed 1 December 2024).

54. District Court of Amsterdam, 11 March 2021, ECLI:NL:RBAMS:2021:1020.

55. Tietosuojavaltuutetun toimisto (Finnish DPA) 22 Marhc 2023, Case
    10048/182/20.

56. EU Digital Services Act (DSA): Regulation (EU) 2022/2065 of the
    European Parliament and of the Council of 19 October 2022 on a
    Single Market for Digital Services and amending Directive 2000/31/EC
    (Digital Services Act), OJ 2022 L 277/1.

57. European Commission, ‘Commission Staff Working Document Executive
    Summary of the Impact Assessment Report Accompanying the Document
    Proposal for a Regulation of the European Parliament and the Council
    on a Single Market for Digital Services (Digital Services Act) and
    Amending Directive 2000/31/EC’, 2020,
    https://eur-lex.europa.eu/legal-content/EN/
    ALL/?uri=CELEX%3A52020SC0349 (accessed 1 December 2024).

58. DMA, OJ 2022 L 265.

59. DMA, OJ 2022 L 265, recital 59.

60. DMA, OJ 2022 L 265, recital 59.

61. Gernot Rieder, ‘Tracing Big Data Imaginaries through Public Policy:
    The Case of the European Commission’, in *The Politics and Policies
    of Big Data: Big Data, Big Brother?* ed. Ann R. Sætnan, Ingrid
    Schneider, and Nicola Green, 89–109 (Routledge, 2018).

62. European Commission, ‘Commission Staff Working Document Impact
    Assessment Report Accompanying the Document Proposal for a
    Regulation of the European Parliament and of the Council on
    Harmonised Rules on Fair Access to and Use of Data (Data Act)’,
    2022, https://eur-lex.europa.eu/legal-

> content/EN/TXT/?uri=CELEX%3A52022SC0034 (accessed 1 December 2024).

1.  Alex Bowyer, Jack Holt, Josephine Go Jefferies, Rob Wilson, David
    Kirk, and Jan David, ‘Human–GDPR Interaction: Practical Experiences
    of Accessing Personal Data’, *CHI ’22: Proceedings of the 2022 CHI
    Conference on Human Factors in Computing Systems* (2022), DOI:
    https://dl.acm.org/ doi/10.1145/3491102.3501947.

2.  Directorate-General for Justice and Consumers (European Commission)
    and Kantar, *The General Data Protection Regulation: Report*
    (Publications Office of the European Union, 2019).

3.  M. Castells, *The Rise of the Network Society*, vol. 1, 2nd ed.
    (Blackwell Publishers, 2009 \[1996\]), 18.

4.  boyd and Crawford, ‘Critical Questions for Big Data’.

5.  ‘Data Studies’, Aarhus University, https://kursuskatalog.au.dk/en/
    course/112914/Data-studies (accessed 25 August 2023).

6.  ‘Datafication of Society’, Aarhus University,
    https://kursuskatalog.au.dk/en/
    course/110859/Datafication-of-Society (accessed 25 August 2023).

7.  ‘Digital Living’, Aarhus University,
    https://kandidat.au.dk/informations videnskab (accessed 25 August
    2023).

8.  For example, ‘Hvad er dine rettigheder’, Datatilsynet, https://www.
    datatilsynet.dk/borger/hvad-er-dine-rettigheder (accessed 25
    August 2023) and ‘For the Public’, Information Commissioner’s
    Office, https://ico.org.uk/ for-the-public (accessed 25 August
    2023).

9.  J. Ausloos and P. Dewitte, ‘Shattering One-Way Mirrors: Data Subject
    Access Rights in Practice’, 20 January 2018,
    https://papers.ssrn.com/ abstract=3106632 (accessed 25 August 2023).

10. R. Mahieu, H. Asghari, and M. van Eeten, ‘Collectively Exercising
    the Right of Access: Individual Effort, Societal Effect’, 13 July
    2018, https://papers. ssrn.com/abstract=3216615 (accessed 25 August
    2023).

11. Clive Norris, Paul de Hert, and Xavier L’Hoiry, and Antonella
    Galetta (eds.), *The Unaccountable State of Surveillance: Exercising
    Access Rights in Europe*, vol. 34 (Springer International
    Publishing, 2017).

12. For a historical perspective, see M. F. Jensen, ‘Statebuilding,
    Establishing Rule of Law and Fighting Corruption in Denmark,
    1660–1900’, in *Anticorruption in History: From Antiquity to the
    Modern Era*, ed. Ronald Kroeze, André Vitória and Guy Geltner,
    197–210 (Oxford University Press 2017). For a quantitative ranking,
    see World Justice Project, *Rule of Law Index 2022* (World Justice
    Project, 2022), https://worldjusticeproject.org/sites/default/
    files/documents/WJPIndex2022.pdf (accessed 1 December 2024).

13. European Foundation for the Improvement of Living and Working
    Conditions, *Societal Change and Trust in Institutions* (Eurofound,
    2018).

14. boyd and Crawford, ‘Critical Questions for Big Data’.

15. Jose van Dijck, ‘Datafication, Dataism and Dataveillance: Big Data
    between Scientific Paradigm and Ideology’, *Surveillance and
    Society* 197, no. 12 (2014): 197–208.

16. For example, Leanne Bowler, Amelia Acker, Wei Jeng, and Yu Chi, ‘“It
    Lives All around Us”: Aspects of Data Literacy in Teen’s Lives’,
    *Proceedings of the Association for Information Science and
    Technology* 54, no. 1 (2017): 27–35 and R. W. Erwin, ‘Data Literacy:
    Real-World Learning through Problemsolving with Data Sets’,
    *American Secondary Education* 18, no. 43 (2015): 18–26.

17. K. D. Haggerty and R. V. Ericson, ‘The Surveillant Assemblage’, in
    *Surveillance, Crime and Social Control*, ed. Clive Norris and Dean
    Wilson, 61–78 (Routledge, 2006).

18. S. Jasanoff and S. H. Kim, *Dreamscapes of Modernity: Sociotechnical
    Imaginaries and the Fabrication of Power* (University of Chicago
    Press, 2015).

19. boyd and Crawford, ‘Critical Questions for Big Data’.

20. Case C-154/21 *RW v Österreichische Post AG* \[2023\] ECR I-1.

21. S. Flensburg and S. Lomborg, ‘Datafication Research: Mapping the
    Field for a Future Agenda’, *New Media and Society* 25, no. 6
    (2021), DOI: https://doi. org/10.1177/14614448211046616.

22. G. C. Bowker and S. L. Star, *Sorting Things Out: Classification and
    Its Consequences* (MIT Press, 1999).

23. Jean-Christophe Plantin, Carl Lagoze, Paul N. Edwards, and Christian
    Sandvig, ‘Infrastructure Studies Meet Platform Studies in the Age of
    Google and Facebook’, *New Media and Society* 293, no. 20 (2018),
    DOI: https://doi. org/10.1177/1461444816661553.

24. J. C. Plantin, ‘Data Cleaners for Pristine Datasets: Visibility and
    Invisibility of Data Processors in Social Science’, *Science,
    Technology, and Human Values* 44, no. 1 (2019): 52–73.

25. G. C. Bowker, *Memory Practices in the Sciences* (paperback edition)
    (MIT Press, 2008); L. Gitelman, *Raw Data Is an Oxymoron* (MIT
    Press, 2013).

26. Bowker and Star, *Sorting Things Out.*

27. VADER and TextBlob are two popular natural language processing
    libraries that include functions which calculate the sentiment of a
    text string. These libraries are trained on different data and use
    different ways to calculate sentiment, so the output when applied to
    the same text is almost always different.

28. EfficientNetV2 and ResNeXt are two popular pre-trained image
    classification models. When given an image as an input, it provides
    a list of different words that the model believes describe the
    content of the image and confidence scores that represent how
    strongly the model thinks that description matches with the image.

29. K. van Es, M. Wieringa, and M. T. Schäfer, ‘Tool Criticism: From
    Digital Methods to Digital Methodology’, *WS.2 2018: Proceedings of
    the 2nd International Conference on Web Studies* (2018), DOI:
    http://dl.acm.org/ citation.cfm?doid=3240431.3240436.

30. Abeba Birhane, Pratyusha Kalluri, Dallas Card, William Agnew, Ravit
    Dotan, and Michelle Bao, ‘The Values Encoded in Machine Learning
    Research’, *FAccT ’22: Proceedings of the 2022 ACM Conference on
    Fairness, Accountability, and Transparency* (2022), DOI:
    https://dl.acm.org/ doi/10.1145/3531146.3533083.

31. M. Hildebrandt, ‘Privacy as Protection of the Incomputable Self:
    From Agnostic to Agonistic Machine Learning’, *Theoretical Inquiries
    in Law* 83, no. 20 (2019): 83–121.

32. H. Korsgaard, C. N. Klokmose, and S. Bødker, ‘Computational
    Alternatives in Participatory Design: Putting the t Back in
    Socio-Technical Research’, *PDC ’16: Proceedings of the 14th
    Participatory Design Conference* (2016), DOI:
    http://dl.acm.org/citation.cfm?doid=2940299.2940314.

33. Joint Research Centre (European Commission), *The EU Digital Markets
    Act: A Report from a Panel of Economic Experts* (Publications Office
    of the European Union, 2021), DOI:
    https://data.europa.eu/doi/10.2760/139337.

34. L. Pangrazio and J. Sefton-Green, ‘The Social Utility of “Data
    Literacy”’, *Learning, Media and Technology* 208, no. 45 (2020):
    208–220.

35. Jansen F, ‘Critical Is Not Political: The Need to (Re)Politicize
    Data Literacy’, *Seminar.net* 17, no. 2 (2021),
    https://journals.oslomet.no/index.php/ seminar/article/view/4280
    (accessed 25 August 2023).

36. J. Elisa Raffaghelli, ‘Is Data Literacy a Catalyst of Social
    Justice? A Response from Nine Data Literacy Initiatives in Higher
    Education’, *Education Sciences* 10, no. 9 (2020), DOI:
    https://doi.org/10.3390/educsci10090233.
